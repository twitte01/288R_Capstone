{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Audio using Spectrograms and CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note on Data locations\n",
    "\n",
    "I recommend putting the audio (WAV) and image (PNG) files in `data\\audio` and `data\\iamges` directories, respectively.\n",
    "\n",
    "The `data\\` directory is already included in the `.gitignore` file, and so these large binary files won't be included in commits.\n",
    "\n",
    "### Example:\n",
    "\n",
    "<img src=\"data_structure_example.png\" style=\"widht:400px; height:auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import IPython.display as ipd\n",
    "from timeit import default_timer as timer\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "\n",
    "import librosa\n",
    "import librosa.display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AudioFile class\n",
    "\n",
    "- `file_path`: Path to the audio file\n",
    "- `file_name`: Name of the audio file (extracted from the path)\n",
    "- `label`: Label of the audio file (derived from the parent directory name)\n",
    "- `audio`: Loaded audio data\n",
    "- `sample_rate`: Sampling rate of the audio file\n",
    "- `duration`: Duration of the audio file in seconds\n",
    "\n",
    "### Methods\n",
    "\n",
    "- `display_waveform()`: Display the waveform of the audio file\n",
    "- `play()`: Play the audio file and return an audio player widget\n",
    "- `trim(top_db=30)`: Trim silent parts of the audio using a decibel threshold\n",
    "- `create_spectrogram()`: Generate a mel spectrogram of the audio file\n",
    "- `show_spectrogram()`: Display the spectrogram of the audio file\n",
    "- `save_spectrogram(output_dir=None, skip_existing=True)`: Save the spectrogram as a PNG file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioFile:\n",
    "    \"\"\"\n",
    "    A class to handle audio files and provide utilities for analysis and visualization.\n",
    "\n",
    "    Attributes:\n",
    "        file_path (str): Path to the audio file.\n",
    "        file_name (str): Name of the audio file (extracted from the path).\n",
    "        label (str): Label of the audio file (derived from the parent directory name).\n",
    "        audio (np.ndarray): Loaded audio data.\n",
    "        sample_rate (int): Sampling rate of the audio file.\n",
    "        duration (float): Duration of the audio file in seconds.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, file_path):\n",
    "        \"\"\"\n",
    "        Initialize the AudioFile instance by loading the audio file and extracting metadata.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): Path to the audio file.\n",
    "        \"\"\"\n",
    "        self.file_path = file_path\n",
    "        self.file_name = os.path.basename(file_path)\n",
    "        self.label = os.path.basename(os.path.dirname(self.file_path))\n",
    "        self.audio, self.sample_rate = librosa.load(file_path)\n",
    "        self.audio = librosa.util.normalize(self.audio)   # normalize audio\n",
    "        self.duration = librosa.get_duration(y=self.audio, sr=self.sample_rate)\n",
    "\n",
    "    def display_waveform(self):\n",
    "        \"\"\"\n",
    "        Display the waveform of the audio file.\n",
    "        \"\"\"\n",
    "        librosa.display.waveshow(self.audio, sr=self.sample_rate)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "    def play(self):\n",
    "        \"\"\"\n",
    "        Play the audio file.\n",
    "\n",
    "        Returns:\n",
    "            IPython.display.Audio: audio player widget.\n",
    "        \"\"\"\n",
    "        return ipd.display(ipd.Audio(self.audio, rate=self.sample_rate))\n",
    "\n",
    "    def trim(self, top_db=50):\n",
    "        \"\"\"\n",
    "        Trim silent parts of the audio based on a decibel threshold.\n",
    "\n",
    "        Args:\n",
    "            top_db (int, optional): Decibel threshold below which audio is considered silent. Defaults to 30.\n",
    "        \"\"\"\n",
    "        self.audio, _ = librosa.effects.trim(self.audio, top_db=top_db)\n",
    "\n",
    "    def create_spectrogram(self):\n",
    "        \"\"\"\n",
    "        Create a mel spectrogram of the audio file.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The mel spectrogram in decibel units.\n",
    "        \"\"\"\n",
    "        mel_scale_sgram = librosa.feature.melspectrogram(\n",
    "            y=self.audio,\n",
    "            sr=self.sample_rate,\n",
    "            power=1)\n",
    "        mel_sgram = librosa.amplitude_to_db(mel_scale_sgram, ref=np.min)\n",
    "        return mel_sgram\n",
    "\n",
    "    def display_spectrogram(self):\n",
    "        \"\"\"\n",
    "        Display the spectrogram of the audio file.\n",
    "        \"\"\"\n",
    "        _spectrogram = self.create_spectrogram()\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        img = librosa.display.specshow(\n",
    "            _spectrogram,\n",
    "            sr=self.sample_rate,\n",
    "            x_axis='time',\n",
    "            y_axis='mel',\n",
    "            ax=ax)\n",
    "        plt.colorbar(img, format='%+2.0f dB')\n",
    "\n",
    "        # remove whitespace around image\n",
    "        plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
    "\n",
    "        plt.show()\n",
    "        plt.close(fig)\n",
    "\n",
    "    def save_spectrogram(self, output_dir=None, skip_existing=True):\n",
    "        \"\"\"\n",
    "        Save the spectrogram as a PNG file.\n",
    "\n",
    "        Args:\n",
    "            output_dir (str, optional): Directory to save the spectrogram. Defaults to the directory of the audio file.\n",
    "            skip_existing (bool, optional): Whether to skip saving if the file already exists. Defaults to True.\n",
    "        \"\"\"\n",
    "        if not output_dir:\n",
    "            output_dir = os.path.dirname(self.file_path)\n",
    "        else:\n",
    "            output_dir = os.path.join(output_dir, self.label)\n",
    "\n",
    "        base, _ = os.path.splitext(self.file_name)\n",
    "        output_file = os.path.join(output_dir, base + \".png\")\n",
    "\n",
    "        if skip_existing and os.path.exists(output_file):\n",
    "            return\n",
    "\n",
    "        spectrogram = self.create_spectrogram()\n",
    "        librosa.display.specshow(spectrogram, sr=self.sample_rate)\n",
    "\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        # save, removing whitespace\n",
    "        plt.savefig(output_file, bbox_inches='tight', pad_inches=0)\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of using AudioFile class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_audio_file = os.path.join(\"data\", \"audio\", \"Speech Commands\", \"backward\", \"0a2b400e_nohash_0.wav\")\n",
    "test_audio = AudioFile(_audio_file)\n",
    "\n",
    "test_audio.display_waveform()\n",
    "test_audio.display_spectrogram()\n",
    "test_audio.trim(top_db=50)\n",
    "test_audio.play()\n",
    "test_audio.display_waveform()\n",
    "test_audio.display_spectrogram()\n",
    "test_audio.play()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Audio Files to Spectrograms\n",
    "\n",
    " - set input_dir and output_dir accordingly\n",
    " - call process_directory()\n",
    " - if skip_existing is True, existing spectrogram PNG files will be skipped (recommended)\n",
    "\n",
    "\n",
    "### NOTE:\n",
    "\n",
    "- Only run this cell if you need to save out all the spectrograms. It takes awhile, and is prone to crashing (hence the use of skip_existing, so it can continue where it left off).\n",
    "- Commented out the \"process_directory(...)\" line at the bottom to avoid accidental runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_directory(input_dir, skip_existing=True, include_trimmed=False):\n",
    "    output_dir = os.path.join(\"data\", \"images\", os.path.basename(input_dir))\n",
    "    output_dir_trimmed = os.path.join(output_dir + \" (trimmed)\")\n",
    "    for root, dirs, files in os.walk(input_dir):\n",
    "        # sort directories alphabetically\n",
    "        dirs.sort()\n",
    "        directory = os.path.basename(root)\n",
    "        print(f\"Processing directory: {directory}\")\n",
    "        for file in files:\n",
    "            if file.endswith('.wav'):\n",
    "                audio = None\n",
    "                # trim off .wav from file\n",
    "                base, _ = os.path.splitext(file)\n",
    "                output_file = os.path.join(output_dir, directory, base + \".png\")\n",
    "                \n",
    "                if not (skip_existing and os.path.exists(output_file)):\n",
    "                    # load file\n",
    "                    audio = AudioFile(os.path.join(root, file))\n",
    "                    # save spectrogram\n",
    "                    audio.save_spectrogram(output_dir, skip_existing=skip_existing)\n",
    "\n",
    "                if include_trimmed:\n",
    "                    output_file_trimmed = os.path.join(output_dir_trimmed, directory, base + \".png\")\n",
    "                    if not (skip_existing and os.path.exists(output_file_trimmed)):\n",
    "                        # have we loaded the file already?\n",
    "                        if not audio:\n",
    "                            audio = AudioFile(os.path.join(root, file))\n",
    "                        # trim and save\n",
    "                        audio.trim()\n",
    "                        audio.save_spectrogram(output_file_trimmed, skip_existing=skip_existing)\n",
    "                \n",
    "process_directory('data/audio/Speech Commands_noise', skip_existing=True, include_trimmed=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corrupt image check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Define your directories\n",
    "output_dir = os.path.join(\"data\", \"images\", \"Speech Commands\")\n",
    "output_dir_trimmed = os.path.join(\"data\", \"images\", \"Speech Commands (trimmed)\")\n",
    "\n",
    "def check_png_corruption(directories, output_file=\"corrupt_pngs.txt\"):\n",
    "    corrupt_files = []\n",
    "    for directory in directories:\n",
    "        for root, dirs, files in os.walk(directory):\n",
    "            for file in files:\n",
    "                if file.lower().endswith('.png'):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        with Image.open(file_path) as img:\n",
    "                            img.verify()  # This will raise an exception if the file is corrupted.\n",
    "                    except Exception as e:\n",
    "                        print(f\"Corrupted PNG found: {file_path} (Error: {e})\")\n",
    "                        corrupt_files.append(file_path)\n",
    "\n",
    "    # Write the results to a text file\n",
    "    if corrupt_files:\n",
    "        with open(output_file, \"w\") as f:\n",
    "            for filename in corrupt_files:\n",
    "                f.write(f\"{filename}\\n\")\n",
    "        print(f\"Found {len(corrupt_files)} corrupt PNG(s). Details saved to '{output_file}'.\")\n",
    "    else:\n",
    "        print(\"No corrupt PNG files found.\")\n",
    "\n",
    "# List of directories to check\n",
    "directories_to_check = [output_dir, output_dir_trimmed]\n",
    "\n",
    "# Run the check\n",
    "check_png_corruption(directories_to_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Pre-process TODO*\n",
    "\n",
    "- spectrogram: look into librosa specshow options. remove black bar at top of many. check axes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"  # Apple Metal Performance Shaders\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# setup tensorboard\n",
    "writer_path = f\"./logs/run_{datetime.datetime.now().strftime('%Y%m%d-%H%M')}\"\n",
    "writer = SummaryWriter(writer_path)\n",
    "\n",
    "model_path = f\"./models/cnn_ryan/{datetime.datetime.now().strftime('%Y%m%d-%H%M')}\"\n",
    "            \n",
    "\n",
    "## PARAMETERS ##\n",
    "image_size = (256, 190) # from 496x369. Closely maintains aspect ratio\n",
    "num_channels = 3 # RGB images\n",
    "# for DataLoader\n",
    "batch_size = 256\n",
    "num_workers = 4\n",
    "# for training\n",
    "accuracy_threshold = 96\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_spectrogram(img):\n",
    "    img = img / 2 + 0.5     # de-normalize\n",
    "    plt.axis('off')\n",
    "    plt.imshow(np.transpose(img.numpy(), (1, 2, 0)))\n",
    "\n",
    "def display_spectrogram_batches(batches, writer_path=None):\n",
    "    _iter = iter(batches)\n",
    "    images, _ = next(_iter)\n",
    "    img_grid = torchvision.utils.make_grid(images)\n",
    "    display_spectrogram(img_grid)\n",
    "\n",
    "    if writer_path:\n",
    "        # write to tensorboard\n",
    "        writer.add_images(writer_path, img_grid.unsqueeze(0))\n",
    "\n",
    "def calc_convolution_output(image_size: Tuple[int, int], kernel_size, stride=1, padding=0):\n",
    "    _w = int((image_size[0] - kernel_size[0] + 2 * padding) / stride) + 1\n",
    "    _h = int((image_size[1] - kernel_size[1] + 2 * padding) / stride) + 1\n",
    "    return (_w, _h)\n",
    "\n",
    "def calc_pooling_output(image_size: Tuple[int, int], kernel_size, stride=None):\n",
    "    if stride is None:\n",
    "        stride = kernel_size\n",
    "    _w = int((image_size[0] - kernel_size[0]) / stride[0]) + 1\n",
    "    _h = int((image_size[1] - kernel_size[1]) / stride[1]) + 1\n",
    "    return (_w, _h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Transform and Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((image_size[0], image_size[1])), # convert to square that can easily divide evenly\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# ImageFolder will load data from subdirectories and assign integer labels\n",
    "data_dir = \"data/images/Speech Commands\"\n",
    "dataset = datasets.ImageFolder(root=data_dir, transform=data_transforms)\n",
    "\n",
    "# get class labels\n",
    "class_labels = [label for label in dataset.class_to_idx]\n",
    "num_classes = len(class_labels)\n",
    "print(f\"Classes: {num_classes}\")\n",
    "\n",
    "# split into training and validation sets\n",
    "val_split = 0.2\n",
    "val_size = int(val_split * len(dataset))\n",
    "train_size = len(dataset) - val_size\n",
    "\n",
    "X_train, X_val = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "print(f\"Training: {train_size}\\nValidation: {val_size}\")\n",
    "\n",
    "# load batches\n",
    "train_batches = torch.utils.data.DataLoader(\n",
    "    X_train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True)\n",
    "\n",
    "val_batches = torch.utils.data.DataLoader(\n",
    "    X_val,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True)\n",
    "\n",
    "# show a spectrogram\n",
    "# display_spectrogram(X_train[0][0])\n",
    "\n",
    "# show some spectrograms\n",
    "# display_spectrogram_batches(val_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Neural Net Class__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 16, kernel_size=1, stride=1, padding=0),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)),\n",
    "            nn.Conv2d(16, 10, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2)),\n",
    "            nn.Conv2d(10, 10, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(in_features=480, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        try:\n",
    "            logits = self.classifier(x)\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: Linear block in_features needs to be: {x.shape[1]}\")\n",
    "\n",
    "        return logits\n",
    "\n",
    "    \n",
    "model_old = CNN_1()\n",
    "print(summary(model_old, input_size=(num_channels, image_size[0], image_size[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectrogramCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SpectrogramCNN, self).__init__()\n",
    "        \n",
    "        # Kernel calculations\n",
    "        # W: input dimension; F: filter size; P: padding; S: stride\n",
    "        # [(W - f + 2p) / s] + 1\n",
    "         \n",
    "        # input: (3, 256, 190)\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=2, padding=1)\n",
    "        _map_size = calc_convolution_output(image_size, kernel_size=self.conv1.kernel_size, stride=self.conv1.stride[0], padding=self.conv1.padding[0])\n",
    "        self.instance_norm = nn.InstanceNorm2d(64)    # speaker normalization\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        _map_size = calc_pooling_output(_map_size, kernel_size=self.pool1.kernel_size)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        _map_size = calc_convolution_output(_map_size, kernel_size=self.conv2.kernel_size, stride=self.conv2.stride[0], padding=self.conv2.padding[0])\n",
    "        self.batch_norm_2 = nn.BatchNorm2d(128)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        _map_size = calc_pooling_output(_map_size, kernel_size=self.pool2.kernel_size)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "        _map_size = calc_convolution_output(_map_size, kernel_size=self.conv3.kernel_size, stride=self.conv3.stride[0], padding=self.conv3.padding[0])\n",
    "        self.batch_norm_3 = nn.BatchNorm2d(256)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        _map_size = calc_pooling_output(_map_size, kernel_size=self.pool3.kernel_size)\n",
    "        \n",
    "        # final feature dimensions - channels x width x height\n",
    "        feature_dims = 256 * _map_size[0] * _map_size[1]\n",
    "        print(f'feature dimensions: {feature_dims}')\n",
    "        \n",
    "        # Fully Connected with larger intermediate layers\n",
    "        self.fc1 = nn.Linear(in_features=feature_dims, out_features=512)\n",
    "        self.fc2 = nn.Linear(in_features=512, out_features=256)\n",
    "        self.fc3 = nn.Linear(in_features=256, out_features=num_classes)\n",
    "        \n",
    "        # dropout for regularization\n",
    "        self.dropout2d = nn.Dropout2d(0.2)  # spatial dropout for conv layers\n",
    "        self.dropout = nn.Dropout(0.4)      # regular dropout for FC layers\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # block 1\n",
    "        x = F.relu(self.instance_norm(self.conv1(x)))  # using speaker normalization here\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout2d(x)\n",
    "        \n",
    "        # block 2\n",
    "        x = F.relu(self.batch_norm_2(self.conv2(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2d(x)\n",
    "        \n",
    "        # block 3\n",
    "        x = F.relu(self.batch_norm_3(self.conv3(x)))\n",
    "        x = self.pool3(x)\n",
    "        x = self.dropout2d(x)\n",
    "        \n",
    "        # flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # FC layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "\n",
    "model = SpectrogramCNN(num_classes)\n",
    "print(summary(model, input_size=(num_channels, image_size[0], image_size[1])))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display training time\n",
    "def display_training_time(start, end):\n",
    "    total_time = end - start\n",
    "    print(f\"Training time : {total_time:.3f} seconds\")\n",
    "    return total_time\n",
    "\n",
    "# display training info for each epoch\n",
    "def display_training_info(epoch, val_loss, train_loss, accuracy):\n",
    "    val_loss = round(val_loss.item(), 2)\n",
    "    train_loss = round(train_loss.item(), 2)\n",
    "    accuracy = round(accuracy, 2)\n",
    "    print(f\"\\nEpoch: {epoch} | Training loss: {train_loss} | Validation loss: {val_loss} | Accuracy: {accuracy}%\")\n",
    "    \n",
    "# calculate accuracy\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc\n",
    "\n",
    "# training\n",
    "def train_neural_net(epochs, model, loss_func, optimizer, train_batches, val_batches, patience=5):\n",
    "    final_accuracy = 0\n",
    "    best_val_accuracy = 0\n",
    "    epochs_without_improvement = 0\n",
    "\n",
    "    # path for saved models\n",
    "    model_path = f\"./models/cnn_ryan/{datetime.datetime.now().strftime('%Y%m%d-%H%M')}\"\n",
    "            \n",
    "    for epoch in tqdm(range(epochs), desc=\"Epochs\"):\n",
    "        # training mode\n",
    "        model.train()\n",
    "        with torch.enable_grad():\n",
    "            train_loss = 0\n",
    "            for images, labels in tqdm(train_batches, desc=\"Training Batches\", leave=False):\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                predictions = model(images)\n",
    "                loss = loss_func(predictions, labels)\n",
    "                train_loss += loss\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            train_loss /= len(train_batches)\n",
    "\n",
    "        # evaluation mode\n",
    "        val_loss, val_accuracy = 0, 0\n",
    "        model.eval()\n",
    "        with torch.inference_mode():\n",
    "            for images, labels in tqdm(val_batches, desc=\"Validation Batches\", leave=False):\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                predictions = model(images)\n",
    "                val_loss += loss_func(predictions, labels)\n",
    "                val_accuracy += accuracy_fn(y_true=labels, y_pred=predictions.argmax(dim=1))\n",
    "            val_loss /= len(val_batches)\n",
    "            val_accuracy /= len(val_batches)\n",
    "            final_accuracy = val_accuracy\n",
    "        display_training_info(epoch+1, val_loss, train_loss, val_accuracy)\n",
    "\n",
    "        # write to tensorboard\n",
    "        writer.add_scalars(\"Loss\", {\n",
    "            \"Training\": train_loss,\n",
    "            \"Validation\": val_loss\n",
    "        }, epoch)\n",
    "        writer.add_scalar(\"Accuracy\", val_accuracy, epoch)\n",
    "\n",
    "        # save models if validation accuracy improves\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            os.makedirs(model_path, exist_ok=True)\n",
    "            torch.save(model.state_dict(), f\"{model_path}/model_params.pt\")\n",
    "            torch.save(model, f\"{model_path}/model_full.pt\")\n",
    "            print(f\"\\nCheckpoint saved at epoch {epoch+1} with val accuracy {val_accuracy:.2f}%\")\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            print(f\"No improvement for {epochs_without_improvement} epoch(s)\")\n",
    "\n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(\"Early stopping: no improvement in validation accuracy for \", patience, \"epochs.\")\n",
    "            break\n",
    "        \n",
    "        if val_accuracy >= accuracy_threshold:\n",
    "            print(\"Early stopping: accuracy threshold reached.\")\n",
    "            break\n",
    "        \n",
    "    return final_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 50\n",
    "learning_rate = 0.001\n",
    "gradient_momentum = 0.9 # only for SGD optimizer\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=gradient_momentum)\n",
    "\n",
    "train_time_start_on_gpu = timer()\n",
    "model_accuracy = train_neural_net(max_epochs, model, loss_func, optimizer, train_batches, val_batches, patience=5)\n",
    "print(f\"\\nTraining complete : {model_accuracy} %\")\n",
    "display_training_time(start=train_time_start_on_gpu, end=timer())\n",
    "\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
