{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">NOTE ABOUT RED CELLS:</span>\n",
    "\n",
    "Some of the cells below have <span style=\"color:red\">red markdown headers</span>, in particular cells that:\n",
    "* convert all the audio WAV files to Spectrogram images (and then check for corrupt images)\n",
    "* model training\n",
    "* model fine-tuning\n",
    "\n",
    "***These are very slow***, so while they work and can be tested, go through the notebook and skip those particular cells, and everything else will still work (as models were saved to checkpoints, etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from timeit import default_timer as timer\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchsummary import summary\n",
    "\n",
    "# add directory to path so we can import modules from src\n",
    "import sys\n",
    "sys.path.append(\"../src\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AudioFile class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from team14 import AudioFile\n",
    "\n",
    "_audio_file = '../data/audio/Speech Commands/forward/0a2b400e_nohash_0.wav'\n",
    "test_audio = AudioFile(_audio_file)\n",
    "\n",
    "test_audio.play()\n",
    "test_audio.display_waveform()\n",
    "test_audio.display_spectrogram()\n",
    "test_audio.trim(top_db=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">Convert Audio Files to Spectrograms</span>\n",
    "\n",
    " - set input_dir and output_dir accordingly\n",
    " - call process_directory()\n",
    " - if skip_existing is True, existing spectrogram PNG files will be skipped (recommended)\n",
    "\n",
    "\n",
    "___NOTE___:\n",
    "\n",
    "- Only run this cell if you need to save out all the spectrograms. It takes awhile, and is prone to crashing (hence the use of skip_existing, so it can continue where it left off).\n",
    "- Commented out the \"process_directory(...)\" line at the bottom to avoid accidental runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_directory(input_dir, skip_existing=True, include_trimmed=False):\n",
    "    output_dir = os.path.join(\"data\", \"images\", os.path.basename(input_dir))\n",
    "    output_dir_trimmed = os.path.join(output_dir + \" (trimmed)\")\n",
    "    for root, dirs, files in os.walk(input_dir):\n",
    "        # sort directories alphabetically\n",
    "        dirs.sort()\n",
    "        directory = os.path.basename(root)\n",
    "        print(f\"Processing directory: {directory}\")\n",
    "        for file in files:\n",
    "            if file.endswith('.wav'):\n",
    "                audio = None\n",
    "                # trim off .wav from file\n",
    "                base, _ = os.path.splitext(file)\n",
    "                output_file = os.path.join(output_dir, directory, base + \".png\")\n",
    "                \n",
    "                if not (skip_existing and os.path.exists(output_file)):\n",
    "                    # load file\n",
    "                    audio = AudioFile(os.path.join(root, file))\n",
    "                    # save spectrogram\n",
    "                    audio.save_spectrogram(output_dir, skip_existing=skip_existing)\n",
    "\n",
    "                if include_trimmed:\n",
    "                    output_file_trimmed = os.path.join(output_dir_trimmed, directory, base + \".png\")\n",
    "                    if not (skip_existing and os.path.exists(output_file_trimmed)):\n",
    "                        # have we loaded the file already?\n",
    "                        if not audio:\n",
    "                            audio = AudioFile(os.path.join(root, file))\n",
    "                        # trim and save\n",
    "                        audio.trim()\n",
    "                        audio.save_spectrogram(output_file_trimmed, skip_existing=skip_existing)\n",
    "                \n",
    "process_directory('data/audio/Speech Commands_noise', skip_existing=True, include_trimmed=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:red\">Corrupt image check</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your directories\n",
    "output_dir = os.path.join(\"data\", \"images\", \"Speech Commands\")\n",
    "output_dir_trimmed = os.path.join(\"data\", \"images\", \"Speech Commands (trimmed)\")\n",
    "\n",
    "def check_png_corruption(directories, output_file=\"corrupt_pngs.txt\"):\n",
    "    corrupt_files = []\n",
    "    for directory in directories:\n",
    "        for root, dirs, files in os.walk(directory):\n",
    "            for file in files:\n",
    "                if file.lower().endswith('.png'):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        with Image.open(file_path) as img:\n",
    "                            img.verify()  # This will raise an exception if the file is corrupted.\n",
    "                    except Exception as e:\n",
    "                        print(f\"Corrupted PNG found: {file_path} (Error: {e})\")\n",
    "                        corrupt_files.append(file_path)\n",
    "\n",
    "    # Write the results to a text file\n",
    "    if corrupt_files:\n",
    "        with open(output_file, \"w\") as f:\n",
    "            for filename in corrupt_files:\n",
    "                f.write(f\"{filename}\\n\")\n",
    "        print(f\"Found {len(corrupt_files)} corrupt PNG(s). Details saved to '{output_file}'.\")\n",
    "    else:\n",
    "        print(\"No corrupt PNG files found.\")\n",
    "\n",
    "# List of directories to check\n",
    "directories_to_check = [output_dir, output_dir_trimmed]\n",
    "\n",
    "# Run the check\n",
    "check_png_corruption(directories_to_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"  # Apple Metal Performance Shaders\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "\n",
    "datetime_stamp = datetime.datetime.now().strftime('%y%m%d-%H%M')\n",
    "\n",
    "# setup tensorboard\n",
    "writer_path = f\"../logs/run_{datetime_stamp}\"\n",
    "writer = SummaryWriter(writer_path)\n",
    "\n",
    "# path to save model\n",
    "model_path = f\"../models/cnn_ryan/{datetime_stamp}\"\n",
    "            \n",
    "\n",
    "## PARAMETERS ##\n",
    "image_size = (256, 190) # from 496x369. Closely maintains aspect ratio\n",
    "num_channels = 3 # RGB images\n",
    "# for DataLoader\n",
    "batch_size = 64\n",
    "num_workers = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform and Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from team14 import SpeechCommandsDataset\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((image_size[0], image_size[1])),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "\n",
    "# load splits from text files\n",
    "with open('../docs/training_list.txt', 'r') as f:\n",
    "    training_list = f.read().splitlines()\n",
    "\n",
    "with open('../docs/validation_list.txt', 'r') as f:\n",
    "    validation_list = f.read().splitlines()\n",
    "\n",
    "with open('../docs/testing_list.txt', 'r') as f:\n",
    "    testing_list = f.read().splitlines()\n",
    "\n",
    "# set dataset root directory\n",
    "data_dir = \"../data/images/Speech Commands (trimmed)\"\n",
    "\n",
    "# create datasets\n",
    "train_dataset = SpeechCommandsDataset(training_list, data_dir, transform=data_transforms)\n",
    "val_dataset = SpeechCommandsDataset(validation_list, data_dir, transform=data_transforms)\n",
    "test_dataset = SpeechCommandsDataset(testing_list, data_dir, transform=data_transforms)\n",
    "\n",
    "# print data sizes\n",
    "print(f\"Training set size: {len(train_dataset)}\")\n",
    "print(f\"Validation set size: {len(val_dataset)}\")\n",
    "print(f\"Testing set size: {len(test_dataset)}\")\n",
    "\n",
    "# create data loaders\n",
    "train_batches = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "val_batches = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "test_batches = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "# get class labels\n",
    "class_labels = train_dataset.class_to_idx\n",
    "num_classes = len(class_labels)\n",
    "print(f\"\\nClasses: {num_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Noisy Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set dataset root directory\n",
    "noisy_data_dir = \"../data/images/Speech Commands_noise\"\n",
    "\n",
    "# create datasets\n",
    "noisy_train_dataset = SpeechCommandsDataset(training_list, noisy_data_dir, transform=data_transforms)\n",
    "noisy_val_dataset = SpeechCommandsDataset(validation_list, noisy_data_dir, transform=data_transforms)\n",
    "noisy_test_dataset = SpeechCommandsDataset(testing_list, noisy_data_dir, transform=data_transforms)\n",
    "\n",
    "# print data sizes\n",
    "print(f\"Training set size: {len(noisy_train_dataset)}\")\n",
    "print(f\"Validation set size: {len(noisy_val_dataset)}\")\n",
    "print(f\"Testing set size: {len(noisy_test_dataset)}\")\n",
    "\n",
    "# create data loaders\n",
    "noisy_train_batches = DataLoader(noisy_train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "noisy_val_batches = DataLoader(noisy_val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "noisy_test_batches = DataLoader(noisy_test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet-style Neural Net Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from team14.models.ryan_resnet import ResNet, BasicBlock\n",
    "\n",
    "# 19 layers, using BasicBlock\n",
    "model = ResNet(BasicBlock, [2, 2, 2, 2], num_classes=num_classes)\n",
    "print(summary(model, input_size=(num_channels, image_size[0], image_size[1])))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display training time\n",
    "def display_training_time(start, end):\n",
    "    total_time = end - start\n",
    "    print(f\"Training time : {total_time:.3f} seconds\")\n",
    "    return total_time\n",
    "    \n",
    "# calculate accuracy\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc\n",
    "\n",
    "# training\n",
    "def train_neural_net(epochs, model, loss_func, optimizer, train_batches, stop_patience=4, lr_scheduler=None, fine_tune=False, **val_datasets):\n",
    "    final_accuracies = {}\n",
    "    best_val_accuracies = {val_name: 0 for val_name in val_datasets.keys()}\n",
    "    epochs_without_improvement = 0\n",
    "    primary_val_name = list(val_datasets.keys())[0]   # use first validation set for early stopping\n",
    "\n",
    "    # check if fine-tuning. if so, freeze some layers\n",
    "    if fine_tune:\n",
    "        trainable_params = 0\n",
    "        total_params = 0\n",
    "        for name, param in model.named_parameters():\n",
    "            if any(layer in name for layer in ['stem', 'layer1', 'layer2']):\n",
    "                param.requires_grad = False\n",
    "            else:\n",
    "                param.requires_grad = True\n",
    "                trainable_params += param.numel()\n",
    "\n",
    "            total_params += param.numel()\n",
    "        print(f\"Fine-tuning with {trainable_params:,} trainable parameters out of {total_params:,} total parameters ({100*trainable_params/total_params:.2f}%)\")\n",
    "\n",
    "\n",
    "    # loop over epochs\n",
    "    for epoch in tqdm(range(epochs), desc=\"Epochs\"):\n",
    "        # === training ===\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for images, labels in tqdm(train_batches, desc=\"Training Batches\", leave=False):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            predictions = model(images)\n",
    "            loss = loss_func(predictions, labels)\n",
    "            train_loss += loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # update learning rate here if using OneCycleLR\n",
    "            if lr_scheduler and isinstance(lr_scheduler, torch.optim.lr_scheduler.OneCycleLR):\n",
    "                lr_scheduler.step()\n",
    "        train_loss /= len(train_batches)\n",
    "\n",
    "        # === evaluation ===\n",
    "        val_metrics = {}\n",
    "        model.eval()\n",
    "        with torch.inference_mode():\n",
    "            for val_name, val_batches in val_datasets.items():   # there might be multiple validation sets    \n",
    "                val_loss = 0\n",
    "                val_accuracy = 0\n",
    "                for images, labels in tqdm(val_batches, desc=f\"{val_name} Batches\", leave=False):\n",
    "                    images = images.to(device)\n",
    "                    labels = labels.to(device)\n",
    "                    predictions = model(images)\n",
    "                    val_loss += loss_func(predictions, labels)\n",
    "                    val_accuracy += accuracy_fn(y_true=labels, y_pred=predictions.argmax(dim=1))\n",
    "\n",
    "                val_loss /= len(val_batches)\n",
    "                val_accuracy /= len(val_batches)\n",
    "                val_metrics[val_name] = {\"loss\": val_loss, \"accuracy\": val_accuracy}\n",
    "                final_accuracies[val_name] = val_accuracy\n",
    "\n",
    "\n",
    "        # update learning rate here if using ReduceLROnPlateau\n",
    "        if lr_scheduler and isinstance(lr_scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            lr_scheduler.step(val_metrics[primary_val_name][\"loss\"])\n",
    "\n",
    "        # get current learning rate            \n",
    "        learning_rate = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        # display metrics\n",
    "        print(f\"\\nEpoch: {epoch+1} | Training loss: {train_loss:.3f} | LR: {learning_rate:.4g}\")\n",
    "        for val_name, metrics in val_metrics.items():\n",
    "            print(f\"   {val_name} loss: {metrics['loss']:.3f} | Accuracy: {metrics['accuracy']:.3f}%\")\n",
    "\n",
    "        # Log to TensorBoard\n",
    "        writer.add_scalar(\"Loss/Training\", train_loss, epoch)\n",
    "        for val_name, metrics in val_metrics.items():\n",
    "            writer.add_scalar(f\"Loss/Validation/{val_name}\", metrics[\"loss\"], epoch)\n",
    "            writer.add_scalar(f\"Accuracy/Validation/{val_name}\", metrics[\"accuracy\"], epoch)\n",
    "        writer.add_scalar(\"Learning Rate\", learning_rate, epoch)\n",
    "\n",
    "        # save checkpoint\n",
    "        os.makedirs(model_path, exist_ok=True)\n",
    "        torch.save(model, f\"{model_path}/rn19_epoch_{epoch+1}.pt\")\n",
    "        print(f\"Checkpoint saved at epoch {epoch+1} with val accuracy {val_accuracy:.2f}%\")\n",
    "\n",
    "        # stop training if validation accuracy has not improved for stop_patience epochs\n",
    "        primary_val_accuracy = val_metrics[primary_val_name][\"accuracy\"]\n",
    "        if primary_val_accuracy > best_val_accuracies[primary_val_name]:\n",
    "            best_val_accuracies[primary_val_name] = primary_val_accuracy\n",
    "            epochs_without_improvement = 0  # reset counter\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            print(f\"No improvement on {primary_val_name} for {epochs_without_improvement} epoch(s)\")\n",
    "\n",
    "        if epochs_without_improvement >= stop_patience:\n",
    "            print(f\"Early stopping: {primary_val_name} accuracy not improved for {stop_patience} epochs.\")\n",
    "            break\n",
    "\n",
    "    return final_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">Training</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 30\n",
    "stop_patience = 10   # if no improvement in validation accuracy for this many epochs, stop training\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# parameters for OneCycleLR\n",
    "max_learning_rate = 1e-1\n",
    "steps_per_epoch = len(train_batches)\n",
    "total_steps = max_epochs * steps_per_epoch  # total number of training steps\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=max_learning_rate, total_steps=total_steps)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5)\n",
    "\n",
    "train_time_start_on_gpu = timer()\n",
    "final_accuracies = train_neural_net(max_epochs, model, loss_func, optimizer, train_batches, stop_patience, lr_scheduler, fine_tune=False, validation=val_batches)\n",
    "\n",
    "print(f\"\\nTraining complete:\")\n",
    "for val_name, accuracy in final_accuracies.items():\n",
    "    print(f\"  - {val_name} accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "display_training_time(start=train_time_start_on_gpu, end=timer())\n",
    "\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a torch model\n",
    "saved_model = torch.load(\"../models/cnn_ryan/250307-0336-full/best_model.pt\", weights_only=False)\n",
    "saved_model.to(device)\n",
    "\n",
    "# set the model to evaluation mode to disable dropout, etc.\n",
    "saved_model.eval()\n",
    "\n",
    "# class labels\n",
    "classes = list(class_labels.keys())\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "all_filenames = []\n",
    "\n",
    "# run inference on the test set\n",
    "with torch.no_grad():\n",
    "    for images, labels, filenames in tqdm(test_batches, desc=\"Test Batches\"):\n",
    "        images = images.to(device)\n",
    "        predictions = saved_model(images)\n",
    "        # store predictions, labels, and filenames\n",
    "        all_preds.extend(predictions.argmax(dim=1).cpu().numpy())\n",
    "        all_labels.extend(labels)\n",
    "        all_filenames.extend(filenames)\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# calculate accuracy score\n",
    "accuracy_test = np.sum(np.array(all_preds) == np.array(all_labels)) / len(all_labels)\n",
    "print(f\"Test Accuracy: {100 * accuracy_test:0.3f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get class index-to-name mapping\n",
    "idx_to_class = {v: k for k, v in class_labels.items()}  # Reverse mapping\n",
    "\n",
    "cm_no_diag = cm.copy()\n",
    "np.fill_diagonal(cm_no_diag, 0)  # Remove diagonal values (correct classifications)\n",
    "\n",
    "# Get the top 5 most confused word pairs (sorted by value)\n",
    "confused_pairs = []\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        if i != j and cm_no_diag[i, j] > 0:  # Non-diagonal misclassifications\n",
    "            confused_pairs.append((i, j, cm_no_diag[i, j]))\n",
    "\n",
    "# Sort by number of misclassifications (desc)\n",
    "confused_pairs = sorted(confused_pairs, key=lambda x: x[2], reverse=True)[:6]\n",
    "confused_pairs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from team14 import NiftyConfusionMatrix\n",
    "\n",
    "cm_plot = NiftyConfusionMatrix(cm, classes)\n",
    "cm_plot.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Confusion Matrix to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cm_plot.to_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference on Noisy Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds_noisy = []\n",
    "all_labels_noisy = []\n",
    "all_filenames = []\n",
    "\n",
    "# run inference on the noisy test set\n",
    "with torch.no_grad():\n",
    "    for images, labels, filenames in tqdm(noisy_test_batches, desc=\"Noisy Image Batches\"):\n",
    "        images = images.to(device)\n",
    "        predictions = saved_model(images)\n",
    "        # store predictions, labels, and filenames\n",
    "        all_preds_noisy.extend(predictions.argmax(dim=1).cpu().numpy())\n",
    "        all_labels_noisy.extend(labels)\n",
    "        all_filenames.extend(filenames)\n",
    "\n",
    "noisy_cm = confusion_matrix(all_labels_noisy, all_preds_noisy)\n",
    "\n",
    "# calculate accuracy score\n",
    "accuracy_noisy = np.sum(np.array(all_preds_noisy) == np.array(all_labels_noisy)) / len(all_labels_noisy)\n",
    "print(f\"Noisy Image Accuracy: {100 * accuracy_noisy:0.3f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_cm_plot = NiftyConfusionMatrix(noisy_cm, classes)\n",
    "my_cm_plot.display(title=\"Noisy Image Confusion Matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">Fine-Tuning on both Original and Noisy Images</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "max_epochs = 25\n",
    "stop_patience = 10   # if no improvement in validation accuracy for this many epochs, stop training\n",
    "learning_rate = 1e-4    # lower loearning rate for fine-tuning\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# combine base and noisy images for fine-tuning\n",
    "combined_train_dataset = ConcatDataset([train_batches.dataset, noisy_train_batches.dataset])\n",
    "combined_train_batches = DataLoader(combined_train_dataset, batch_size=train_batches.batch_size, shuffle=True)\n",
    "\n",
    "train_time_start_on_gpu = timer()\n",
    "final_accuracies = train_neural_net(max_epochs, model, loss_func, optimizer, combined_train_batches, stop_patience, lr_scheduler=None, fine_tune=True, val_noisy=noisy_val_batches, val_baseline=val_batches)\n",
    "\n",
    "print(f\"\\nTraining complete:\")\n",
    "for val_name, accuracy in final_accuracies.items():\n",
    "    print(f\"  - {val_name} accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "display_training_time(start=train_time_start_on_gpu, end=timer())\n",
    "\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning Metrics - Plotly Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "def plotly_metrics(csv_path):\n",
    "\n",
    "    loss_train_path = os.path.join(csv_path, \"loss_train.csv\")\n",
    "    loss_val_baseline_path = os.path.join(csv_path, \"loss_val_baseline.csv\")\n",
    "    loss_val_noisy_path = os.path.join(csv_path, \"loss_val_noisy.csv\")\n",
    "    val_baseline_path = os.path.join(csv_path, \"val_baseline.csv\")\n",
    "    val_noisy_path = os.path.join(csv_path, \"val_noisy.csv\")\n",
    "\n",
    "    # Load CSV files\n",
    "    loss_train = pd.read_csv(loss_train_path)\n",
    "    loss_val_baseline = pd.read_csv(loss_val_baseline_path)\n",
    "    loss_val_noisy = pd.read_csv(loss_val_noisy_path)\n",
    "    val_baseline = pd.read_csv(val_baseline_path)\n",
    "    val_noisy = pd.read_csv(val_noisy_path)\n",
    "\n",
    "    # Epochs for x-axis (start from 1)\n",
    "    epochs = list(range(1, len(loss_train) + 1))\n",
    "\n",
    "    # ----- LOSS FIGURE -----\n",
    "    fig_loss = go.Figure()\n",
    "\n",
    "    fig_loss.add_trace(go.Scatter(\n",
    "        x=epochs, y=loss_train['Value'], mode='lines+markers', name='Train',\n",
    "        line=dict(color='#8884d8', width=2), hovertemplate=\"%{y:.3f}\"\n",
    "    ))\n",
    "\n",
    "    fig_loss.add_trace(go.Scatter(\n",
    "        x=epochs, y=loss_val_baseline['Value'], mode='lines+markers', name='Baseline Validation',\n",
    "        line=dict(color='#82ca9d', width=2), hovertemplate=\"%{y:.3f}\"\n",
    "    ))\n",
    "\n",
    "    fig_loss.add_trace(go.Scatter(\n",
    "        x=epochs, y=loss_val_noisy['Value'], mode='lines+markers', name='Noisy Validation',\n",
    "        line=dict(color='#ff7300', width=2), hovertemplate=\"%{y:.3f}\"\n",
    "    ))\n",
    "\n",
    "    fig_loss.update_layout(\n",
    "        title_text=\"Fine-tuning Loss\", xaxis_title=\"Epoch\", yaxis_title=\"Loss\", hovermode=\"x unified\",\n",
    "        width=800, height=500,\n",
    "        legend=dict(\n",
    "            orientation=\"h\", yanchor=\"top\", y=-0.15, xanchor=\"center\", x=0.5\n",
    "        )\n",
    "    )\n",
    "    # Show Loss Figure\n",
    "    fig_loss.show()\n",
    "\n",
    "    # ----- ACCURACY FIGURE -----\n",
    "    fig_acc = go.Figure()\n",
    "\n",
    "    fig_acc.add_trace(go.Scatter(\n",
    "        x=epochs, y=val_baseline['Value'], mode='lines+markers', name='Baseline',\n",
    "        line=dict(color='#82ca9d', width=2), hovertemplate=\"%{y:.2f}%\"\n",
    "    ))\n",
    "\n",
    "    fig_acc.add_trace(go.Scatter(\n",
    "        x=epochs, y=val_noisy['Value'], mode='lines+markers', name='Noisy',\n",
    "        line=dict(color='#ff7300', width=2), hovertemplate=\"%{y:.2f}%\"\n",
    "    ))\n",
    "\n",
    "    fig_acc.update_layout(\n",
    "        title_text=\"Fine-tuning Accuracy\", xaxis_title=\"Epoch\", yaxis_title=\"Accuracy\", hovermode=\"x unified\",\n",
    "        width=800, height=500,\n",
    "        legend=dict(\n",
    "            orientation=\"h\", yanchor=\"top\", y=-0.15, xanchor=\"center\", x=0.5\n",
    "        )\n",
    "    )\n",
    "    # Show Accuracy Figure\n",
    "    fig_acc.show()\n",
    "\n",
    "csv_path = \"../models/cnn_ryan/250312-finetune-full/csv_data\"\n",
    "plotly_metrics(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation after fine-tuning on Original and Noisy Images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a torch model\n",
    "model_finetune_full = torch.load(\"../models/cnn_ryan/250312-finetune-full/best_model.pt\", weights_only=False)\n",
    "model_finetune_full.to(device)\n",
    "model_finetune_full.eval()\n",
    "classes = list(class_labels.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Original Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "# run inference on the test set\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_batches, desc=\"Test Batches\"):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        predictions = model_finetune_full(images)\n",
    "        all_preds.extend(predictions.argmax(dim=1).cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "cm_finetune_full_baseline = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# calculate accuracy score\n",
    "accuracy_test = np.sum(np.array(all_preds) == np.array(all_labels)) / len(all_labels)\n",
    "print(f\"Test Accuracy (original images): {100 * accuracy_test:0.3f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_cm_plot = NiftyConfusionMatrix(noisy_cm, classes)\n",
    "my_cm_plot.display(title=\"Original Images on Fine-tuned Model (using Original + Noisy images)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Noisy Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "# run inference on the test set\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(noisy_test_batches, desc=\"Noisy Image Batches\"):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        predictions = model_finetune_full(images)\n",
    "        all_preds.extend(predictions.argmax(dim=1).cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "cm_finetune_full_noisy = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# calculate accuracy score\n",
    "accuracy_test = np.sum(np.array(all_preds) == np.array(all_labels)) / len(all_labels)\n",
    "print(f\"Test Accuracy (noisy images): {100 * accuracy_test:0.3f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_cm_plot = NiftyConfusionMatrix(noisy_cm, classes)\n",
    "my_cm_plot.display(title=\"Noisy Images on Fine-tuned Model (using Original + Noisy images)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
