{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Audio using Spectrograms and CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note on Data locations\n",
    "\n",
    "I recommend putting the audio (WAV) and image (PNG) files in `data\\audio` and `data\\iamges` directories, respectively.\n",
    "\n",
    "The `data\\` directory is already included in the `.gitignore` file, and so these large binary files won't be included in commits.\n",
    "\n",
    "### Example:\n",
    "\n",
    "<img src=\"data_structure_example.png\" style=\"widht:400px; height:auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import IPython.display as ipd\n",
    "from timeit import default_timer as timer\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "from nnViewer import wrap_model, run_gui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AudioFile class\n",
    "\n",
    "- `file_path`: Path to the audio file\n",
    "- `file_name`: Name of the audio file (extracted from the path)\n",
    "- `label`: Label of the audio file (derived from the parent directory name)\n",
    "- `audio`: Loaded audio data\n",
    "- `sample_rate`: Sampling rate of the audio file\n",
    "- `duration`: Duration of the audio file in seconds\n",
    "\n",
    "### Methods\n",
    "\n",
    "- `display_waveform()`: Display the waveform of the audio file\n",
    "- `play()`: Play the audio file and return an audio player widget\n",
    "- `trim(top_db=30)`: Trim silent parts of the audio using a decibel threshold\n",
    "- `create_spectrogram()`: Generate a mel spectrogram of the audio file\n",
    "- `show_spectrogram()`: Display the spectrogram of the audio file\n",
    "- `save_spectrogram(output_dir=None, skip_existing=True)`: Save the spectrogram as a PNG file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioFile:\n",
    "    \"\"\"\n",
    "    A class to handle audio files and provide utilities for analysis and visualization.\n",
    "\n",
    "    Attributes:\n",
    "        file_path (str): Path to the audio file.\n",
    "        file_name (str): Name of the audio file (extracted from the path).\n",
    "        label (str): Label of the audio file (derived from the parent directory name).\n",
    "        audio (np.ndarray): Loaded audio data.\n",
    "        sample_rate (int): Sampling rate of the audio file.\n",
    "        duration (float): Duration of the audio file in seconds.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, file_path):\n",
    "        \"\"\"\n",
    "        Initialize the AudioFile instance by loading the audio file and extracting metadata.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): Path to the audio file.\n",
    "        \"\"\"\n",
    "        self.file_path = file_path\n",
    "        self.file_name = os.path.basename(file_path)\n",
    "        self.label = os.path.basename(os.path.dirname(self.file_path))\n",
    "        self.audio, self.sample_rate = librosa.load(file_path)\n",
    "        self.audio = librosa.util.normalize(self.audio)   # normalize audio\n",
    "        self.duration = librosa.get_duration(y=self.audio, sr=self.sample_rate)\n",
    "\n",
    "    def display_waveform(self):\n",
    "        \"\"\"\n",
    "        Display the waveform of the audio file.\n",
    "        \"\"\"\n",
    "        librosa.display.waveshow(self.audio, sr=self.sample_rate)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "    def play(self):\n",
    "        \"\"\"\n",
    "        Play the audio file.\n",
    "\n",
    "        Returns:\n",
    "            IPython.display.Audio: audio player widget.\n",
    "        \"\"\"\n",
    "        return ipd.display(ipd.Audio(self.audio, rate=self.sample_rate))\n",
    "\n",
    "    def trim(self, top_db=50):\n",
    "        \"\"\"\n",
    "        Trim silent parts of the audio based on a decibel threshold.\n",
    "\n",
    "        Args:\n",
    "            top_db (int, optional): Decibel threshold below which audio is considered silent. Defaults to 30.\n",
    "        \"\"\"\n",
    "        self.audio, _ = librosa.effects.trim(self.audio, top_db=top_db)\n",
    "\n",
    "    def create_spectrogram(self):\n",
    "        \"\"\"\n",
    "        Create a mel spectrogram of the audio file.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The mel spectrogram in decibel units.\n",
    "        \"\"\"\n",
    "        mel_scale_sgram = librosa.feature.melspectrogram(\n",
    "            y=self.audio,\n",
    "            sr=self.sample_rate,\n",
    "            power=1)\n",
    "        mel_sgram = librosa.amplitude_to_db(mel_scale_sgram, ref=np.min)\n",
    "        return mel_sgram\n",
    "\n",
    "    def display_spectrogram(self):\n",
    "        \"\"\"\n",
    "        Display the spectrogram of the audio file.\n",
    "        \"\"\"\n",
    "        _spectrogram = self.create_spectrogram()\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        img = librosa.display.specshow(\n",
    "            _spectrogram,\n",
    "            sr=self.sample_rate,\n",
    "            x_axis='time',\n",
    "            y_axis='mel',\n",
    "            ax=ax)\n",
    "        plt.colorbar(img, format='%+2.0f dB')\n",
    "\n",
    "        # remove whitespace around image\n",
    "        plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
    "\n",
    "        plt.show()\n",
    "        plt.close(fig)\n",
    "\n",
    "    def save_spectrogram(self, output_dir=None, skip_existing=True):\n",
    "        \"\"\"\n",
    "        Save the spectrogram as a PNG file.\n",
    "\n",
    "        Args:\n",
    "            output_dir (str, optional): Directory to save the spectrogram. Defaults to the directory of the audio file.\n",
    "            skip_existing (bool, optional): Whether to skip saving if the file already exists. Defaults to True.\n",
    "        \"\"\"\n",
    "        if not output_dir:\n",
    "            output_dir = os.path.dirname(self.file_path)\n",
    "        else:\n",
    "            output_dir = os.path.join(output_dir, self.label)\n",
    "\n",
    "        base, _ = os.path.splitext(self.file_name)\n",
    "        output_file = os.path.join(output_dir, base + \".png\")\n",
    "\n",
    "        if skip_existing and os.path.exists(output_file):\n",
    "            return\n",
    "\n",
    "        spectrogram = self.create_spectrogram()\n",
    "        librosa.display.specshow(spectrogram, sr=self.sample_rate)\n",
    "\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        # save, removing whitespace\n",
    "        plt.savefig(output_file, bbox_inches='tight', pad_inches=0)\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of using AudioFile class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_audio_file = os.path.join(\"data\", \"audio\", \"Speech Commands\", \"backward\", \"0a2b400e_nohash_0.wav\")\n",
    "test_audio = AudioFile(_audio_file)\n",
    "\n",
    "test_audio.display_waveform()\n",
    "test_audio.display_spectrogram()\n",
    "test_audio.trim(top_db=50)\n",
    "test_audio.play()\n",
    "test_audio.display_waveform()\n",
    "test_audio.display_spectrogram()\n",
    "test_audio.play()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Audio Files to Spectrograms\n",
    "\n",
    " - set input_dir and output_dir accordingly\n",
    " - call process_directory()\n",
    " - if skip_existing is True, existing spectrogram PNG files will be skipped (recommended)\n",
    "\n",
    "\n",
    "### NOTE:\n",
    "\n",
    "- Only run this cell if you need to save out all the spectrograms. It takes awhile, and is prone to crashing (hence the use of skip_existing, so it can continue where it left off).\n",
    "- Commented out the \"process_directory(...)\" line at the bottom to avoid accidental runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_directory(input_dir, skip_existing=True, include_trimmed=False):\n",
    "    output_dir = os.path.join(\"data\", \"images\", os.path.basename(input_dir))\n",
    "    output_dir_trimmed = os.path.join(output_dir + \" (trimmed)\")\n",
    "    for root, dirs, files in os.walk(input_dir):\n",
    "        # sort directories alphabetically\n",
    "        dirs.sort()\n",
    "        directory = os.path.basename(root)\n",
    "        print(f\"Processing directory: {directory}\")\n",
    "        for file in files:\n",
    "            if file.endswith('.wav'):\n",
    "                audio = None\n",
    "                # trim off .wav from file\n",
    "                base, _ = os.path.splitext(file)\n",
    "                output_file = os.path.join(output_dir, directory, base + \".png\")\n",
    "                \n",
    "                if not (skip_existing and os.path.exists(output_file)):\n",
    "                    # load file\n",
    "                    audio = AudioFile(os.path.join(root, file))\n",
    "                    # save spectrogram\n",
    "                    audio.save_spectrogram(output_dir, skip_existing=skip_existing)\n",
    "\n",
    "                if include_trimmed:\n",
    "                    output_file_trimmed = os.path.join(output_dir_trimmed, directory, base + \".png\")\n",
    "                    if not (skip_existing and os.path.exists(output_file_trimmed)):\n",
    "                        # have we loaded the file already?\n",
    "                        if not audio:\n",
    "                            audio = AudioFile(os.path.join(root, file))\n",
    "                        # trim and save\n",
    "                        audio.trim()\n",
    "                        audio.save_spectrogram(output_file_trimmed, skip_existing=skip_existing)\n",
    "                \n",
    "process_directory('data/audio/Speech Commands_noise', skip_existing=True, include_trimmed=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corrupt image check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Define your directories\n",
    "output_dir = os.path.join(\"data\", \"images\", \"Speech Commands\")\n",
    "output_dir_trimmed = os.path.join(\"data\", \"images\", \"Speech Commands (trimmed)\")\n",
    "\n",
    "def check_png_corruption(directories, output_file=\"corrupt_pngs.txt\"):\n",
    "    corrupt_files = []\n",
    "    for directory in directories:\n",
    "        for root, dirs, files in os.walk(directory):\n",
    "            for file in files:\n",
    "                if file.lower().endswith('.png'):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        with Image.open(file_path) as img:\n",
    "                            img.verify()  # This will raise an exception if the file is corrupted.\n",
    "                    except Exception as e:\n",
    "                        print(f\"Corrupted PNG found: {file_path} (Error: {e})\")\n",
    "                        corrupt_files.append(file_path)\n",
    "\n",
    "    # Write the results to a text file\n",
    "    if corrupt_files:\n",
    "        with open(output_file, \"w\") as f:\n",
    "            for filename in corrupt_files:\n",
    "                f.write(f\"{filename}\\n\")\n",
    "        print(f\"Found {len(corrupt_files)} corrupt PNG(s). Details saved to '{output_file}'.\")\n",
    "    else:\n",
    "        print(\"No corrupt PNG files found.\")\n",
    "\n",
    "# List of directories to check\n",
    "directories_to_check = [output_dir, output_dir_trimmed]\n",
    "\n",
    "# Run the check\n",
    "check_png_corruption(directories_to_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Pre-process TODO*\n",
    "\n",
    "- spectrogram: look into librosa specshow options. remove black bar at top of many. check axes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"  # Apple Metal Performance Shaders\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "\n",
    "datetime_stamp = datetime.datetime.now().strftime('%y%m%d-%H%M')\n",
    "\n",
    "# setup tensorboard\n",
    "writer_path = f\"./logs/run_{datetime_stamp}\"\n",
    "writer = SummaryWriter(writer_path)\n",
    "\n",
    "# path to save model\n",
    "model_path = f\"./models/cnn_ryan/{datetime_stamp}\"\n",
    "            \n",
    "\n",
    "## PARAMETERS ##\n",
    "image_size = (256, 190) # from 496x369. Closely maintains aspect ratio\n",
    "num_channels = 3 # RGB images\n",
    "# for DataLoader\n",
    "batch_size = 64\n",
    "num_workers = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Transform and Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: 35\n",
      "Training: 84664\n",
      "Validation: 21165\n"
     ]
    }
   ],
   "source": [
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((image_size[0], image_size[1])), # convert to square that can easily divide evenly\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# ImageFolder will load data from subdirectories and assign integer labels\n",
    "data_dir = \"data/images/Speech Commands\"\n",
    "dataset = datasets.ImageFolder(root=data_dir, transform=data_transforms)\n",
    "\n",
    "# get class labels\n",
    "class_labels = [label for label in dataset.class_to_idx]\n",
    "num_classes = len(class_labels)\n",
    "print(f\"Classes: {num_classes}\")\n",
    "\n",
    "# for speed of training, we'll only use a subset of the data, randomly selected\n",
    "fraction = 1\n",
    "subset_size = int(len(dataset) * fraction)\n",
    "indices = torch.randperm(len(dataset))[:subset_size]\n",
    "dataset = torch.utils.data.Subset(dataset, indices)\n",
    "\n",
    "\n",
    "# split into training and validation sets\n",
    "val_split = 0.2\n",
    "val_size = int(val_split * len(dataset))\n",
    "train_size = len(dataset) - val_size\n",
    "\n",
    "X_train, X_val = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "print(f\"Training: {train_size}\\nValidation: {val_size}\")\n",
    "\n",
    "# load batches\n",
    "train_batches = torch.utils.data.DataLoader(\n",
    "    X_train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True)\n",
    "\n",
    "val_batches = torch.utils.data.DataLoader(\n",
    "    X_val,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_spectrogram(img):\n",
    "    img = img / 2 + 0.5     # de-normalize\n",
    "    plt.axis('off')\n",
    "    plt.imshow(np.transpose(img.numpy(), (1, 2, 0)))\n",
    "\n",
    "def display_spectrogram_batches(batches, writer_path=None):\n",
    "    _iter = iter(batches)\n",
    "    images, _ = next(_iter)\n",
    "    img_grid = torchvision.utils.make_grid(images)\n",
    "    display_spectrogram(img_grid)\n",
    "\n",
    "    if writer_path:\n",
    "        # write to tensorboard\n",
    "        writer.add_images(writer_path, img_grid.unsqueeze(0))\n",
    "\n",
    "def calc_convolution_output(image_size: Tuple[int, int], kernel_size, stride=1, padding=0):\n",
    "    _w = int((image_size[0] - kernel_size[0] + 2 * padding) / stride) + 1\n",
    "    _h = int((image_size[1] - kernel_size[1] + 2 * padding) / stride) + 1\n",
    "    return (_w, _h)\n",
    "\n",
    "def calc_pooling_output(image_size: Tuple[int, int], kernel_size, stride=None):\n",
    "    if stride is None:\n",
    "        stride = kernel_size\n",
    "    _w = int((image_size[0] - kernel_size[0]) / stride[0]) + 1\n",
    "    _h = int((image_size[1] - kernel_size[1]) / stride[1]) + 1\n",
    "    return (_w, _h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Neural Net Class__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ResNet style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 32, 128, 95]             864\n",
      "       BatchNorm2d-2          [-1, 32, 128, 95]              64\n",
      "              ReLU-3          [-1, 32, 128, 95]               0\n",
      "         ConvLayer-4          [-1, 32, 128, 95]               0\n",
      "            Conv2d-5          [-1, 64, 128, 95]          18,432\n",
      "       BatchNorm2d-6          [-1, 64, 128, 95]             128\n",
      "              ReLU-7          [-1, 64, 128, 95]               0\n",
      "         ConvLayer-8          [-1, 64, 128, 95]               0\n",
      "         MaxPool2d-9           [-1, 64, 64, 48]               0\n",
      "           Conv2d-10           [-1, 64, 64, 48]          36,864\n",
      "      BatchNorm2d-11           [-1, 64, 64, 48]             128\n",
      "           Conv2d-12           [-1, 64, 64, 48]          36,864\n",
      "      BatchNorm2d-13           [-1, 64, 64, 48]             128\n",
      "       BasicBlock-14           [-1, 64, 64, 48]               0\n",
      "           Conv2d-15           [-1, 64, 64, 48]          36,864\n",
      "      BatchNorm2d-16           [-1, 64, 64, 48]             128\n",
      "           Conv2d-17           [-1, 64, 64, 48]          36,864\n",
      "      BatchNorm2d-18           [-1, 64, 64, 48]             128\n",
      "       BasicBlock-19           [-1, 64, 64, 48]               0\n",
      "           Conv2d-20          [-1, 128, 32, 24]          73,728\n",
      "      BatchNorm2d-21          [-1, 128, 32, 24]             256\n",
      "           Conv2d-22          [-1, 128, 32, 24]         147,456\n",
      "      BatchNorm2d-23          [-1, 128, 32, 24]             256\n",
      "           Conv2d-24          [-1, 128, 32, 24]           8,192\n",
      "      BatchNorm2d-25          [-1, 128, 32, 24]             256\n",
      "       BasicBlock-26          [-1, 128, 32, 24]               0\n",
      "           Conv2d-27          [-1, 128, 32, 24]         147,456\n",
      "      BatchNorm2d-28          [-1, 128, 32, 24]             256\n",
      "           Conv2d-29          [-1, 128, 32, 24]         147,456\n",
      "      BatchNorm2d-30          [-1, 128, 32, 24]             256\n",
      "       BasicBlock-31          [-1, 128, 32, 24]               0\n",
      "           Conv2d-32          [-1, 256, 16, 12]         294,912\n",
      "      BatchNorm2d-33          [-1, 256, 16, 12]             512\n",
      "           Conv2d-34          [-1, 256, 16, 12]         589,824\n",
      "      BatchNorm2d-35          [-1, 256, 16, 12]             512\n",
      "           Conv2d-36          [-1, 256, 16, 12]          32,768\n",
      "      BatchNorm2d-37          [-1, 256, 16, 12]             512\n",
      "       BasicBlock-38          [-1, 256, 16, 12]               0\n",
      "           Conv2d-39          [-1, 256, 16, 12]         589,824\n",
      "      BatchNorm2d-40          [-1, 256, 16, 12]             512\n",
      "           Conv2d-41          [-1, 256, 16, 12]         589,824\n",
      "      BatchNorm2d-42          [-1, 256, 16, 12]             512\n",
      "       BasicBlock-43          [-1, 256, 16, 12]               0\n",
      "           Conv2d-44            [-1, 512, 8, 6]       1,179,648\n",
      "      BatchNorm2d-45            [-1, 512, 8, 6]           1,024\n",
      "           Conv2d-46            [-1, 512, 8, 6]       2,359,296\n",
      "      BatchNorm2d-47            [-1, 512, 8, 6]           1,024\n",
      "           Conv2d-48            [-1, 512, 8, 6]         131,072\n",
      "      BatchNorm2d-49            [-1, 512, 8, 6]           1,024\n",
      "       BasicBlock-50            [-1, 512, 8, 6]               0\n",
      "           Conv2d-51            [-1, 512, 8, 6]       2,359,296\n",
      "      BatchNorm2d-52            [-1, 512, 8, 6]           1,024\n",
      "           Conv2d-53            [-1, 512, 8, 6]       2,359,296\n",
      "      BatchNorm2d-54            [-1, 512, 8, 6]           1,024\n",
      "       BasicBlock-55            [-1, 512, 8, 6]               0\n",
      "           Linear-56                   [-1, 35]          17,955\n",
      "================================================================\n",
      "Total params: 11,204,419\n",
      "Trainable params: 11,204,419\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.56\n",
      "Forward/backward pass size (MB): 67.88\n",
      "Params size (MB): 42.74\n",
      "Estimated Total Size (MB): 111.17\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# A simple convolutional layer with BatchNorm and ReLU\n",
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, zero_bn=False):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size,\n",
    "                              stride, padding, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        # If zero_bn is True, initialize the BatchNorm weight (gamma) to zero.\n",
    "        if zero_bn:\n",
    "            nn.init.constant_(self.bn.weight, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.bn(self.conv(x)))\n",
    "\n",
    "# The stem: a sequence of ConvLayers with the first layer doing stride-2,\n",
    "# followed by a max pooling layer.\n",
    "def _resnet_stem(*sizes):\n",
    "    # sizes should be a sequence like (in_channels, mid_channels, out_channels)\n",
    "    stem_layers = [\n",
    "        ConvLayer(sizes[i], sizes[i+1], kernel_size=3, stride=2 if i == 0 else 1, padding=1)\n",
    "        for i in range(len(sizes) - 1)\n",
    "    ]\n",
    "    stem_layers.append(nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "    return stem_layers\n",
    "\n",
    "# Basic residual block with two 3x3 convolutions.\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        # Adjust the shortcut if dimensions differ\n",
    "        if stride != 1 or in_channels != self.expansion * out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, self.expansion * out_channels,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        return F.relu(out)\n",
    "\n",
    "# Bottleneck residual block with a 1x1, 3x3, 1x1 structure.\n",
    "class BottleneckBlock(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        # Reduce channels\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        # 3x3 convolution\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        # Restore channels (multiplying by expansion factor)\n",
    "        self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion,\n",
    "                               kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels * self.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels * self.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * self.expansion)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        return F.relu(out)\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes):\n",
    "        super().__init__()\n",
    "        # Use a stem instead of a single initial convolution:\n",
    "        # For example, a stem from 3 channels -> 32 channels -> 64 channels.\n",
    "        self.stem = nn.Sequential(*_resnet_stem(3, 32, 64))\n",
    "        # After the stem, the output has 64 channels.\n",
    "        self.in_channels = 64\n",
    "\n",
    "        # Residual layers\n",
    "        self.layer1 = self._make_layer(block, 64,  num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "\n",
    "        # Final fully connected layer\n",
    "        self.linear = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.stem(x)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        # Global average pooling; adjust kernel size as needed.\n",
    "        out = F.avg_pool2d(out, out.size()[2:])\n",
    "        out = out.view(out.size(0), -1)\n",
    "        return self.linear(out)\n",
    "\n",
    "# ResNet 18 style, using BasicBlock\n",
    "model18 = ResNet(BasicBlock, [2, 2, 2, 2], num_classes=num_classes)\n",
    "\n",
    "# ResNet 50 style, using BottleneckBlock\n",
    "# model50 = ResNet(BottleneckBlock, [3, 4, 6, 3], num_classes=num_classes)\n",
    "\n",
    "print(summary(model18, input_size=(num_channels, image_size[0], image_size[1])))\n",
    "# print(summary(model50, input_size=(num_channels, image_size[0], image_size[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectrogramCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SpectrogramCNN, self).__init__()\n",
    "        \n",
    "        # Kernel calculations\n",
    "        # W: input dimension; F: filter size; P: padding; S: stride\n",
    "        # [(W - f + 2p) / s] + 1\n",
    "         \n",
    "        # input: (3, 256, 190)\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=2, padding=1)\n",
    "        _map_size = calc_convolution_output(image_size, kernel_size=self.conv1.kernel_size, stride=self.conv1.stride[0], padding=self.conv1.padding[0])\n",
    "        self.instance_norm = nn.InstanceNorm2d(64)    # speaker normalization\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        _map_size = calc_pooling_output(_map_size, kernel_size=self.pool1.kernel_size)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        _map_size = calc_convolution_output(_map_size, kernel_size=self.conv2.kernel_size, stride=self.conv2.stride[0], padding=self.conv2.padding[0])\n",
    "        self.batch_norm_2 = nn.BatchNorm2d(128)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        _map_size = calc_pooling_output(_map_size, kernel_size=self.pool2.kernel_size)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "        _map_size = calc_convolution_output(_map_size, kernel_size=self.conv3.kernel_size, stride=self.conv3.stride[0], padding=self.conv3.padding[0])\n",
    "        self.batch_norm_3 = nn.BatchNorm2d(256)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        _map_size = calc_pooling_output(_map_size, kernel_size=self.pool3.kernel_size)\n",
    "        \n",
    "        # final feature dimensions - channels x width x height\n",
    "        feature_dims = 256 * _map_size[0] * _map_size[1]\n",
    "        print(f'feature dimensions: {feature_dims}')\n",
    "        \n",
    "        # Fully Connected with larger intermediate layers\n",
    "        self.fc1 = nn.Linear(in_features=feature_dims, out_features=512)\n",
    "        self.fc2 = nn.Linear(in_features=512, out_features=256)\n",
    "        self.fc3 = nn.Linear(in_features=256, out_features=num_classes)\n",
    "        \n",
    "        # dropout for regularization\n",
    "        self.dropout2d = nn.Dropout2d(0.2)  # spatial dropout for conv layers\n",
    "        self.dropout = nn.Dropout(0.4)      # regular dropout for FC layers\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # block 1\n",
    "        x = F.relu(self.instance_norm(self.conv1(x)))  # using speaker normalization here\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout2d(x)\n",
    "        \n",
    "        # block 2\n",
    "        x = F.relu(self.batch_norm_2(self.conv2(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2d(x)\n",
    "        \n",
    "        # block 3\n",
    "        x = F.relu(self.batch_norm_3(self.conv3(x)))\n",
    "        x = self.pool3(x)\n",
    "        x = self.dropout2d(x)\n",
    "        \n",
    "        # flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # FC layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "\n",
    "model = SpectrogramCNN(num_classes)\n",
    "print(summary(model, input_size=(num_channels, image_size[0], image_size[1])))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display training time\n",
    "def display_training_time(start, end):\n",
    "    total_time = end - start\n",
    "    print(f\"Training time : {total_time:.3f} seconds\")\n",
    "    return total_time\n",
    "\n",
    "# display training info for each epoch\n",
    "def display_training_info(epoch, val_loss, train_loss, accuracy, learning_rate):\n",
    "    accuracy = round(accuracy, 2)\n",
    "    print(f\"\\nEpoch: {epoch} | Training loss: {train_loss.item():.3f} | Validation loss: {val_loss.item():.3f} | Accuracy: {accuracy:.2f}% | LR: {learning_rate:.4g}\")\n",
    "    \n",
    "# calculate accuracy\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc\n",
    "\n",
    "# training\n",
    "def train_neural_net(epochs, model, loss_func, optimizer, train_batches, val_batches, stop_patience=4, lr_scheduler=None):\n",
    "    final_accuracy = 0\n",
    "    last_val_accuracy = 0\n",
    "    epochs_without_improvement = 0\n",
    "            \n",
    "    for epoch in tqdm(range(epochs), desc=\"Epochs\"):\n",
    "        # === training ===\n",
    "        model.train()\n",
    "        with torch.enable_grad():\n",
    "            train_loss = 0\n",
    "            for images, labels in tqdm(train_batches, desc=\"Training Batches\", leave=False):\n",
    "                labels = labels.to(device)\n",
    "                predictions = model(images.to(device))\n",
    "                loss = loss_func(predictions, labels)\n",
    "                train_loss += loss\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                # update learning rate here if using OneCycleLR\n",
    "                if isinstance(lr_scheduler, torch.optim.lr_scheduler.OneCycleLR):\n",
    "                    lr_scheduler.step()\n",
    "\n",
    "            train_loss /= len(train_batches)\n",
    "\n",
    "\n",
    "        # === evaluation ===\n",
    "        val_loss, val_accuracy = 0, 0\n",
    "        model.eval()\n",
    "        with torch.inference_mode():\n",
    "            for images, labels in tqdm(val_batches, desc=\"Validation Batches\", leave=False):\n",
    "                labels = labels.to(device)\n",
    "                predictions = model(images.to(device))\n",
    "                val_loss += loss_func(predictions, labels)\n",
    "                val_accuracy += accuracy_fn(y_true=labels, y_pred=predictions.argmax(dim=1))\n",
    "\n",
    "            val_loss /= len(val_batches)\n",
    "            val_accuracy /= len(val_batches)\n",
    "            final_accuracy = val_accuracy\n",
    "\n",
    "\n",
    "        # update learning rate here if using ReduceLROnPlateau\n",
    "        if isinstance(lr_scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            lr_scheduler.step(val_loss)\n",
    "\n",
    "        # get current leqrning rate            \n",
    "        learning_rate = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        display_training_info(epoch+1, val_loss, train_loss, val_accuracy, learning_rate)\n",
    "\n",
    "        # log to tensorboard\n",
    "        writer.add_scalars(\"Loss\", {\"Training\": train_loss, \"Validation\": val_loss}, epoch)\n",
    "        writer.add_scalar(\"Accuracy\", val_accuracy, epoch)\n",
    "        writer.add_scalar(\"Learning Rate\", learning_rate, epoch)        \n",
    "\n",
    "        # save model if validation accuracy improves\n",
    "        if val_accuracy > last_val_accuracy:\n",
    "            os.makedirs(model_path, exist_ok=True)\n",
    "            torch.save(model.state_dict(), f\"{model_path}/model_params.pt\")\n",
    "            torch.save(model, f\"{model_path}/model_full.pt\")\n",
    "            print(f\"Checkpoint saved at epoch {epoch+1} with val accuracy {val_accuracy:.2f}%\")\n",
    "            epochs_without_improvement = 0  # reset counter\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            print(f\"No improvement for {epochs_without_improvement} epoch(s)\")\n",
    "\n",
    "        if epochs_without_improvement >= stop_patience:\n",
    "            print(f\"Early stopping: no improvement in validation accuracy for {stop_patience} epochs.\")\n",
    "            break\n",
    "\n",
    "        # update last_val_accuracy\n",
    "        last_val_accuracy = val_accuracy\n",
    "        \n",
    "    return final_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e876f739e024ae283648854f9d389ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daada1e4f3cd4d0084ace5a438f2060f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Batches:   0%|          | 0/1323 [00:02<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b813184861844de6865397ea45286ce3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Batches:   0%|          | 0/331 [00:02<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1 | Training loss: 0.766 | Validation loss: 0.335 | Accuracy: 89.91% | LR: 0.0004411\n",
      "Checkpoint saved at epoch 1 with val accuracy 89.91%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f5256f9774d46e987f8ba193d406912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Batches:   0%|          | 0/1323 [00:02<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac46adaec2b847b28303e4c05f103aad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Batches:   0%|          | 0/331 [00:02<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 2 | Training loss: 0.242 | Validation loss: 0.306 | Accuracy: 90.38% | LR: 0.0005636\n",
      "Checkpoint saved at epoch 2 with val accuracy 90.38%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61aabb6ba8014514933528f626180ca1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Batches:   0%|          | 0/1323 [00:02<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c9f6790192a4af98fc58df6394aea76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Batches:   0%|          | 0/331 [00:02<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 3 | Training loss: 0.196 | Validation loss: 0.222 | Accuracy: 93.04% | LR: 0.0007654\n",
      "Checkpoint saved at epoch 3 with val accuracy 93.04%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59d0538078444beca6c7978909170946",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Batches:   0%|          | 0/1323 [00:02<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5097b3e89484bfa9d340ae12bb5b551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Batches:   0%|          | 0/331 [00:02<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 4 | Training loss: 0.180 | Validation loss: 0.232 | Accuracy: 93.31% | LR: 0.001043\n",
      "Checkpoint saved at epoch 4 with val accuracy 93.31%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80121fc8661c4cb9b5915ba78ff56628",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Batches:   0%|          | 0/1323 [00:02<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c337df86a4540a4a6316a814e5f9696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Batches:   0%|          | 0/331 [00:02<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 5 | Training loss: 0.169 | Validation loss: 0.244 | Accuracy: 92.67% | LR: 0.001392\n",
      "No improvement for 1 epoch(s)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e11d51da919438482a7683d12bbb38b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Batches:   0%|          | 0/1323 [00:02<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3a0e65acbc5455fad4d4237f8158073",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Batches:   0%|          | 0/331 [00:02<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 6 | Training loss: 0.160 | Validation loss: 0.239 | Accuracy: 93.06% | LR: 0.001806\n",
      "Checkpoint saved at epoch 6 with val accuracy 93.06%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae427a5e091f4a6eaee61eb3dfb11946",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Batches:   0%|          | 0/1323 [00:02<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "031d717507cc45b4a8206c49385813a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Batches:   0%|          | 0/331 [00:02<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 7 | Training loss: 0.154 | Validation loss: 0.217 | Accuracy: 93.30% | LR: 0.002278\n",
      "Checkpoint saved at epoch 7 with val accuracy 93.30%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d8344db6ca5458f92af4d295f76aec4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Batches:   0%|          | 0/1323 [00:02<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41d43c208ad645169088a77ddb1aa9bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Batches:   0%|          | 0/331 [00:02<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 8 | Training loss: 0.144 | Validation loss: 0.217 | Accuracy: 93.79% | LR: 0.0028\n",
      "Checkpoint saved at epoch 8 with val accuracy 93.79%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "089ade7c31794fb4bbcc662585de1eba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Batches:   0%|          | 0/1323 [00:02<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "761684ef367c4fb3932ecb874053bf48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Batches:   0%|          | 0/331 [00:02<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 9 | Training loss: 0.132 | Validation loss: 0.262 | Accuracy: 92.75% | LR: 0.003363\n",
      "No improvement for 1 epoch(s)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecb61303f5764a62b809395fccbfaa0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Batches:   0%|          | 0/1323 [00:02<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c43dd9d8d07645768af3ab72a4ea85e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Batches:   0%|          | 0/331 [00:02<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 10 | Training loss: 0.125 | Validation loss: 0.238 | Accuracy: 93.41% | LR: 0.003958\n",
      "Checkpoint saved at epoch 10 with val accuracy 93.41%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b67372569824acd8d46dcdde1d1c688",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Batches:   0%|          | 0/1323 [00:02<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed04f8a286d343a38cb1a283b168633c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Batches:   0%|          | 0/331 [00:03<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 11 | Training loss: 0.117 | Validation loss: 0.255 | Accuracy: 93.52% | LR: 0.004574\n",
      "Checkpoint saved at epoch 11 with val accuracy 93.52%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "846a85ca5dd84c6698bec3bc7e2f72dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Batches:   0%|          | 0/1323 [00:02<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b34096511bed46f5aa9267520b0a8251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Batches:   0%|          | 0/331 [00:03<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 12 | Training loss: 0.108 | Validation loss: 0.202 | Accuracy: 94.34% | LR: 0.0052\n",
      "Checkpoint saved at epoch 12 with val accuracy 94.34%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f9e6ab6ce7a4820b4b727d690651373",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Batches:   0%|          | 0/1323 [00:02<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "038097b4cb594a14ad95f893f7482020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Batches:   0%|          | 0/331 [00:02<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 13 | Training loss: 0.099 | Validation loss: 0.201 | Accuracy: 94.40% | LR: 0.005827\n",
      "Checkpoint saved at epoch 13 with val accuracy 94.40%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f1014aabf7c490a97c3182cf92baf86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Batches:   0%|          | 0/1323 [00:02<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd1eb30ac7644633a67fc9af2b656a71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Batches:   0%|          | 0/331 [00:02<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 14 | Training loss: 0.095 | Validation loss: 0.202 | Accuracy: 94.74% | LR: 0.006443\n",
      "Checkpoint saved at epoch 14 with val accuracy 94.74%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b22e8fdae6364834ba529a113a1c3fef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Batches:   0%|          | 0/1323 [00:02<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b159391d4bde4778b721fd4132492bf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Batches:   0%|          | 0/331 [00:02<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 15 | Training loss: 0.085 | Validation loss: 0.215 | Accuracy: 94.21% | LR: 0.007037\n",
      "No improvement for 1 epoch(s)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "056f47958cad4d4999954f55abe1af84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Batches:   0%|          | 0/1323 [00:02<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a33f082e0414540a909ee6bd1ada1fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Batches:   0%|          | 0/331 [00:02<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 16 | Training loss: 0.083 | Validation loss: 0.189 | Accuracy: 94.81% | LR: 0.0076\n",
      "Checkpoint saved at epoch 16 with val accuracy 94.81%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23c643f135cd43929c297ae8bc6171f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Batches:   0%|          | 0/1323 [00:02<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "790e0f3272354c4090278e078986ac3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Batches:   0%|          | 0/331 [00:02<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 17 | Training loss: 0.071 | Validation loss: 0.199 | Accuracy: 95.04% | LR: 0.008122\n",
      "Checkpoint saved at epoch 17 with val accuracy 95.04%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "475428da8b4c4f1dbf02e450d06355b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Batches:   0%|          | 0/1323 [00:02<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2975194af3674551926349cf571a8b83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Batches:   0%|          | 0/331 [00:02<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 18 | Training loss: 0.069 | Validation loss: 0.202 | Accuracy: 94.93% | LR: 0.008594\n",
      "No improvement for 1 epoch(s)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bf6e29c5e024f5c9eeec7415bdcc7b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Batches:   0%|          | 0/1323 [00:02<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b6cd1bc6f4b470a88fb8e5f30983634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Batches:   0%|          | 0/331 [00:02<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 19 | Training loss: 0.061 | Validation loss: 0.269 | Accuracy: 93.94% | LR: 0.009008\n",
      "No improvement for 2 epoch(s)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d2d12efaf8f4fbda6702d4f9b9412e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Batches:   0%|          | 0/1323 [00:02<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "203e4d5074d24f089e677404d2f4b7d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Batches:   0%|          | 0/331 [00:02<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 20 | Training loss: 0.059 | Validation loss: 0.264 | Accuracy: 93.68% | LR: 0.009357\n",
      "No improvement for 3 epoch(s)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd96eead2aea450cbe996bc1d80db912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Batches:   0%|          | 0/1323 [00:02<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "716f97a8f84b4ee6bd6a8a324cdd0d02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Batches:   0%|          | 0/331 [00:02<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 21 | Training loss: 0.053 | Validation loss: 0.223 | Accuracy: 94.95% | LR: 0.009635\n",
      "Checkpoint saved at epoch 21 with val accuracy 94.95%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99f37a917c5f4135afd946f7c834ffc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Batches:   0%|          | 0/1323 [00:02<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a97fbf77faa4ce3a733d11a21b93d61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Batches:   0%|          | 0/331 [00:02<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 22 | Training loss: 0.047 | Validation loss: 0.186 | Accuracy: 95.69% | LR: 0.009837\n",
      "Checkpoint saved at epoch 22 with val accuracy 95.69%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6980e93939b4eb6ad8cdc1d9087ecbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Batches:   0%|          | 0/1323 [00:02<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd487ef0d1b04a18be59dd6688162a0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Batches:   0%|          | 0/331 [00:02<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 23 | Training loss: 0.044 | Validation loss: 0.216 | Accuracy: 95.17% | LR: 0.009959\n",
      "No improvement for 1 epoch(s)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d033e5406f134d61a71965a3e17e7a0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Batches:   0%|          | 0/1323 [00:02<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcf3eb5663c5424293a6bc5289a3bf72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Batches:   0%|          | 0/331 [00:02<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 24 | Training loss: 0.037 | Validation loss: 0.223 | Accuracy: 95.20% | LR: 0.01\n",
      "Checkpoint saved at epoch 24 with val accuracy 95.20%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe76ab7d364e4d0fba44d758fb80acf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Batches:   0%|          | 0/1323 [00:02<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94a0e1d4981a4020b14ba0c9530840f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Batches:   0%|          | 0/331 [00:02<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 25 | Training loss: 0.035 | Validation loss: 0.214 | Accuracy: 95.61% | LR: 0.009992\n",
      "Checkpoint saved at epoch 25 with val accuracy 95.61%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c82f08d86514ddc9b2692d3baf8997d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Batches:   0%|          | 0/1323 [00:02<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6e9412e394140d5b8e16e8767a5e176",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Batches:   0%|          | 0/331 [00:02<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 26 | Training loss: 0.034 | Validation loss: 0.231 | Accuracy: 95.09% | LR: 0.009969\n",
      "No improvement for 1 epoch(s)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fae6c236f7f4d62ac02095e07cdf0f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Batches:   0%|          | 0/1323 [00:02<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3103c11bc5fe4f89bd9d9a549e4e76dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Batches:   0%|          | 0/331 [00:02<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 27 | Training loss: 0.023 | Validation loss: 0.321 | Accuracy: 94.27% | LR: 0.009929\n",
      "No improvement for 2 epoch(s)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4689cbb11f3848b589e24cbbcf88facb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Batches:   0%|          | 0/1323 [00:02<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b2cdee77b5e443d82c95ca8673bb2c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Batches:   0%|          | 0/331 [00:02<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 28 | Training loss: 0.023 | Validation loss: 0.282 | Accuracy: 95.09% | LR: 0.009875\n",
      "Checkpoint saved at epoch 28 with val accuracy 95.09%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a8f0f414db7499c8b5a25627b7bd398",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Batches:   0%|          | 0/1323 [00:02<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = model18\n",
    "model = model.to(device)\n",
    "\n",
    "max_epochs = 80\n",
    "stop_patience = 6   # if no improvement in validation accuracy for this many epochs, stop training\n",
    "learning_rate = 3e-3\n",
    "\n",
    "# parameters for OneCycleLR\n",
    "max_learning_rate = 1e-2\n",
    "steps_per_epoch = len(train_batches)\n",
    "total_steps = max_epochs * steps_per_epoch  # total number of training steps\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3)\n",
    "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=max_learning_rate, total_steps=total_steps)\n",
    "\n",
    "\n",
    "train_time_start_on_gpu = timer()\n",
    "model_accuracy = train_neural_net(max_epochs, model, loss_func, optimizer, train_batches, val_batches, stop_patience, lr_scheduler)\n",
    "print(f\"\\nTraining complete : {model_accuracy} %\")\n",
    "display_training_time(start=train_time_start_on_gpu, end=timer())\n",
    "\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
