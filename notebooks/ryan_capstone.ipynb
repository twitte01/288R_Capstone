{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note on Data locations\n",
    "\n",
    "I recommend putting the audio (WAV) and image (PNG) files in `data\\audio` and `data\\images` directories, respectively.\n",
    "\n",
    "The `data\\` directory is already included in the `.gitignore` file, and so these large binary files won't be included in commits.\n",
    "\n",
    "### Example:\n",
    "\n",
    "<img src=\"../img/data_structure_example.png\" style=\"widht:400px; height:auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "\n",
    "# add directory to path so we can import modules from src\n",
    "import sys\n",
    "sys.path.append(\"../src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the model using nnViewer's wrap_model function\n",
    "saved_model = torch.load(\"../models/cnn_ryan/250221-0322/model_full.pt\", weights_only=False)\n",
    "device = 'cpu'\n",
    "saved_model.to(device)\n",
    "\n",
    "wrapped_model = wrap_model(saved_model)\n",
    "\n",
    "\n",
    "image_size = (256, 190) # from 496x369. Closely maintains aspect ratio\n",
    "\n",
    "img = Image.open(\"../data/images/Speech Commands/backward/0ba018fc_nohash_1.png\").convert('RGB')\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((image_size[0], image_size[1])),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "img = data_transforms(img).unsqueeze(0)  # [1, 3, 256, 190]\n",
    "\n",
    "saved_model.eval()  # Set the model to evaluation mode to disable dropout, etc.\n",
    "with torch.no_grad():\n",
    "    output = saved_model(img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AudioFile class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from team14.audio_file import AudioFile\n",
    "\n",
    "_audio_file = '../data/audio/Speech Commands/forward/0a2b400e_nohash_0.wav'\n",
    "test_audio = AudioFile(_audio_file)\n",
    "\n",
    "test_audio.display_waveform()\n",
    "test_audio.display_spectrogram()\n",
    "test_audio.trim(top_db=50)\n",
    "test_audio.play()\n",
    "test_audio.display_waveform()\n",
    "test_audio.display_spectrogram()\n",
    "test_audio.play()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Audio Files to Spectrograms\n",
    "\n",
    " - set input_dir and output_dir accordingly\n",
    " - call process_directory()\n",
    " - if skip_existing is True, existing spectrogram PNG files will be skipped (recommended)\n",
    "\n",
    "\n",
    "### NOTE:\n",
    "\n",
    "- Only run this cell if you need to save out all the spectrograms. It takes awhile, and is prone to crashing (hence the use of skip_existing, so it can continue where it left off).\n",
    "- Commented out the \"process_directory(...)\" line at the bottom to avoid accidental runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_directory(input_dir, skip_existing=True, include_trimmed=False):\n",
    "    output_dir = os.path.join(\"data\", \"images\", os.path.basename(input_dir))\n",
    "    output_dir_trimmed = os.path.join(output_dir + \" (trimmed)\")\n",
    "    for root, dirs, files in os.walk(input_dir):\n",
    "        # sort directories alphabetically\n",
    "        dirs.sort()\n",
    "        directory = os.path.basename(root)\n",
    "        print(f\"Processing directory: {directory}\")\n",
    "        for file in files:\n",
    "            if file.endswith('.wav'):\n",
    "                audio = None\n",
    "                # trim off .wav from file\n",
    "                base, _ = os.path.splitext(file)\n",
    "                output_file = os.path.join(output_dir, directory, base + \".png\")\n",
    "                \n",
    "                if not (skip_existing and os.path.exists(output_file)):\n",
    "                    # load file\n",
    "                    audio = AudioFile(os.path.join(root, file))\n",
    "                    # save spectrogram\n",
    "                    audio.save_spectrogram(output_dir, skip_existing=skip_existing)\n",
    "\n",
    "                if include_trimmed:\n",
    "                    output_file_trimmed = os.path.join(output_dir_trimmed, directory, base + \".png\")\n",
    "                    if not (skip_existing and os.path.exists(output_file_trimmed)):\n",
    "                        # have we loaded the file already?\n",
    "                        if not audio:\n",
    "                            audio = AudioFile(os.path.join(root, file))\n",
    "                        # trim and save\n",
    "                        audio.trim()\n",
    "                        audio.save_spectrogram(output_file_trimmed, skip_existing=skip_existing)\n",
    "                \n",
    "process_directory('data/audio/Speech Commands_noise', skip_existing=True, include_trimmed=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corrupt image check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Define your directories\n",
    "output_dir = os.path.join(\"data\", \"images\", \"Speech Commands\")\n",
    "output_dir_trimmed = os.path.join(\"data\", \"images\", \"Speech Commands (trimmed)\")\n",
    "\n",
    "def check_png_corruption(directories, output_file=\"corrupt_pngs.txt\"):\n",
    "    corrupt_files = []\n",
    "    for directory in directories:\n",
    "        for root, dirs, files in os.walk(directory):\n",
    "            for file in files:\n",
    "                if file.lower().endswith('.png'):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        with Image.open(file_path) as img:\n",
    "                            img.verify()  # This will raise an exception if the file is corrupted.\n",
    "                    except Exception as e:\n",
    "                        print(f\"Corrupted PNG found: {file_path} (Error: {e})\")\n",
    "                        corrupt_files.append(file_path)\n",
    "\n",
    "    # Write the results to a text file\n",
    "    if corrupt_files:\n",
    "        with open(output_file, \"w\") as f:\n",
    "            for filename in corrupt_files:\n",
    "                f.write(f\"{filename}\\n\")\n",
    "        print(f\"Found {len(corrupt_files)} corrupt PNG(s). Details saved to '{output_file}'.\")\n",
    "    else:\n",
    "        print(\"No corrupt PNG files found.\")\n",
    "\n",
    "# List of directories to check\n",
    "directories_to_check = [output_dir, output_dir_trimmed]\n",
    "\n",
    "# Run the check\n",
    "check_png_corruption(directories_to_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Pre-process TODO*\n",
    "\n",
    "- spectrogram: look into librosa specshow options. remove black bar at top of many. check axes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"  # Apple Metal Performance Shaders\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "\n",
    "datetime_stamp = datetime.datetime.now().strftime('%y%m%d-%H%M')\n",
    "\n",
    "# setup tensorboard\n",
    "writer_path = f\"../logs/run_{datetime_stamp}\"\n",
    "writer = SummaryWriter(writer_path)\n",
    "\n",
    "# path to save model\n",
    "model_path = f\"../models/cnn_ryan/{datetime_stamp}\"\n",
    "            \n",
    "\n",
    "## PARAMETERS ##\n",
    "image_size = (256, 190) # from 496x369. Closely maintains aspect ratio\n",
    "num_channels = 3 # RGB images\n",
    "# for DataLoader\n",
    "batch_size = 64\n",
    "num_workers = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Transform and Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((image_size[0], image_size[1])), # convert to square that can easily divide evenly\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# ImageFolder will load data from subdirectories and assign integer labels\n",
    "data_dir = \"../data/images/Speech Commands\"\n",
    "dataset = datasets.ImageFolder(root=data_dir, transform=data_transforms)\n",
    "\n",
    "# get class labels\n",
    "class_labels = [label for label in dataset.class_to_idx]\n",
    "num_classes = len(class_labels)\n",
    "print(f\"Classes: {num_classes}\")\n",
    "\n",
    "# for speed of training, we'll only use a subset of the data, randomly selected\n",
    "fraction = 1\n",
    "subset_size = int(len(dataset) * fraction)\n",
    "indices = torch.randperm(len(dataset))[:subset_size]\n",
    "dataset = torch.utils.data.Subset(dataset, indices)\n",
    "\n",
    "\n",
    "# split into training and validation sets\n",
    "val_split = 0.2\n",
    "val_size = int(val_split * len(dataset))\n",
    "train_size = len(dataset) - val_size\n",
    "\n",
    "X_train, X_val = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "print(f\"Training: {train_size}\\nValidation: {val_size}\")\n",
    "\n",
    "# load batches\n",
    "train_batches = torch.utils.data.DataLoader(\n",
    "    X_train,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True)\n",
    "\n",
    "val_batches = torch.utils.data.DataLoader(\n",
    "    X_val,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_spectrogram(img):\n",
    "    img = img / 2 + 0.5     # de-normalize\n",
    "    plt.axis('off')\n",
    "    plt.imshow(np.transpose(img.numpy(), (1, 2, 0)))\n",
    "\n",
    "def display_spectrogram_batches(batches, writer_path=None):\n",
    "    _iter = iter(batches)\n",
    "    images, _ = next(_iter)\n",
    "    img_grid = torchvision.utils.make_grid(images)\n",
    "    display_spectrogram(img_grid)\n",
    "\n",
    "    if writer_path:\n",
    "        # write to tensorboard\n",
    "        writer.add_images(writer_path, img_grid.unsqueeze(0))\n",
    "\n",
    "def calc_convolution_output(image_size: Tuple[int, int], kernel_size, stride=1, padding=0):\n",
    "    _w = int((image_size[0] - kernel_size[0] + 2 * padding) / stride) + 1\n",
    "    _h = int((image_size[1] - kernel_size[1] + 2 * padding) / stride) + 1\n",
    "    return (_w, _h)\n",
    "\n",
    "def calc_pooling_output(image_size: Tuple[int, int], kernel_size, stride=None):\n",
    "    if stride is None:\n",
    "        stride = kernel_size\n",
    "    _w = int((image_size[0] - kernel_size[0]) / stride[0]) + 1\n",
    "    _h = int((image_size[1] - kernel_size[1]) / stride[1]) + 1\n",
    "    return (_w, _h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Neural Net Model__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ResNet style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from team14.models.ryan_resnet import ResNet, BasicBlock, BottleneckBlock\n",
    "\n",
    "# 19 layres, using BasicBlock\n",
    "saved_model = ResNet(BasicBlock, [2, 2, 2, 2], num_classes=num_classes)\n",
    "\n",
    "print(summary(saved_model, input_size=(num_channels, image_size[0], image_size[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display training time\n",
    "def display_training_time(start, end):\n",
    "    total_time = end - start\n",
    "    print(f\"Training time : {total_time:.3f} seconds\")\n",
    "    return total_time\n",
    "\n",
    "# display training info for each epoch\n",
    "def display_training_info(epoch, val_loss, train_loss, accuracy, learning_rate):\n",
    "    accuracy = round(accuracy, 2)\n",
    "    print(f\"\\nEpoch: {epoch} | Training loss: {train_loss.item():.3f} | Validation loss: {val_loss.item():.3f} | Accuracy: {accuracy:.2f}% | LR: {learning_rate:.4g}\")\n",
    "    \n",
    "# calculate accuracy\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc\n",
    "\n",
    "# training\n",
    "def train_neural_net(epochs, model, loss_func, optimizer, train_batches, val_batches, stop_patience=4, lr_scheduler=None):\n",
    "    final_accuracy = 0\n",
    "    last_val_accuracy = 0\n",
    "    epochs_without_improvement = 0\n",
    "            \n",
    "    for epoch in tqdm(range(epochs), desc=\"Epochs\"):\n",
    "        # === training ===\n",
    "        model.train()\n",
    "        with torch.enable_grad():\n",
    "            train_loss = 0\n",
    "            for images, labels in tqdm(train_batches, desc=\"Training Batches\", leave=False):\n",
    "                labels = labels.to(device)\n",
    "                predictions = model(images.to(device))\n",
    "                loss = loss_func(predictions, labels)\n",
    "                train_loss += loss\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                # update learning rate here if using OneCycleLR\n",
    "                if isinstance(lr_scheduler, torch.optim.lr_scheduler.OneCycleLR):\n",
    "                    lr_scheduler.step()\n",
    "\n",
    "            train_loss /= len(train_batches)\n",
    "\n",
    "\n",
    "        # === evaluation ===\n",
    "        val_loss, val_accuracy = 0, 0\n",
    "        model.eval()\n",
    "        with torch.inference_mode():\n",
    "            for images, labels in tqdm(val_batches, desc=\"Validation Batches\", leave=False):\n",
    "                labels = labels.to(device)\n",
    "                predictions = model(images.to(device))\n",
    "                val_loss += loss_func(predictions, labels)\n",
    "                val_accuracy += accuracy_fn(y_true=labels, y_pred=predictions.argmax(dim=1))\n",
    "\n",
    "            val_loss /= len(val_batches)\n",
    "            val_accuracy /= len(val_batches)\n",
    "            final_accuracy = val_accuracy\n",
    "\n",
    "\n",
    "        # update learning rate here if using ReduceLROnPlateau\n",
    "        if isinstance(lr_scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            lr_scheduler.step(val_loss)\n",
    "\n",
    "        # get current leqrning rate            \n",
    "        learning_rate = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        display_training_info(epoch+1, val_loss, train_loss, val_accuracy, learning_rate)\n",
    "\n",
    "        # log to tensorboard\n",
    "        writer.add_scalars(\"Loss\", {\"Training\": train_loss, \"Validation\": val_loss}, epoch)\n",
    "        writer.add_scalar(\"Accuracy\", val_accuracy, epoch)\n",
    "        writer.add_scalar(\"Learning Rate\", learning_rate, epoch)        \n",
    "\n",
    "        # save model if validation accuracy improves\n",
    "        if val_accuracy > last_val_accuracy:\n",
    "            os.makedirs(model_path, exist_ok=True)\n",
    "            torch.save(model.state_dict(), f\"{model_path}/model_params.pt\")\n",
    "            torch.save(model, f\"{model_path}/model_full.pt\")\n",
    "            print(f\"Checkpoint saved at epoch {epoch+1} with val accuracy {val_accuracy:.2f}%\")\n",
    "            epochs_without_improvement = 0  # reset counter\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            print(f\"No improvement for {epochs_without_improvement} epoch(s)\")\n",
    "\n",
    "        if epochs_without_improvement >= stop_patience:\n",
    "            print(f\"Early stopping: no improvement in validation accuracy for {stop_patience} epochs.\")\n",
    "            break\n",
    "\n",
    "        # update last_val_accuracy\n",
    "        last_val_accuracy = val_accuracy\n",
    "        \n",
    "    return final_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model = saved_model.to(device)\n",
    "\n",
    "max_epochs = 50\n",
    "stop_patience = 5   # if no improvement in validation accuracy for this many epochs, stop training\n",
    "learning_rate = 3e-3\n",
    "\n",
    "# parameters for OneCycleLR\n",
    "max_learning_rate = 1e-2\n",
    "steps_per_epoch = len(train_batches)\n",
    "total_steps = max_epochs * steps_per_epoch  # total number of training steps\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(saved_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=max_learning_rate, total_steps=total_steps)\n",
    "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3)\n",
    "\n",
    "\n",
    "train_time_start_on_gpu = timer()\n",
    "model_accuracy = train_neural_net(max_epochs, saved_model, loss_func, optimizer, train_batches, val_batches, stop_patience, lr_scheduler)\n",
    "print(f\"\\nTraining complete : {model_accuracy} %\")\n",
    "display_training_time(start=train_time_start_on_gpu, end=timer())\n",
    "\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference and nnViewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from nnViewer import wrap_model, run_gui\n",
    "from team14.models.ryan_resnet import ResNet\n",
    "\n",
    "\n",
    "# load a torch model\n",
    "saved_model = torch.load(\"../models/cnn_ryan/resnet-style/model_full.pt\", weights_only=False)\n",
    "# move to cpu as otherwise nnViewer won't work\n",
    "device = 'cpu' \n",
    "saved_model.to(device)\n",
    "\n",
    "# Wrap the model using nnViewer's wrap_model function\n",
    "wrapped_model = wrap_model(saved_model)\n",
    "\n",
    "# grab an image\n",
    "img = Image.open(\"../data/images/Speech Commands/backward/0ba018fc_nohash_1.png\").convert('RGB')\n",
    "\n",
    "image_size = (256, 190) # from 496x369. Closely maintains aspect ratio\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((image_size[0], image_size[1])),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "img = data_transforms(img).unsqueeze(0)  # [1, 3, 256, 190]\n",
    "\n",
    "# do a forward pass on the image\n",
    "saved_model.eval()  # Set the model to evaluation mode to disable dropout, etc.\n",
    "with torch.no_grad():\n",
    "    output = saved_model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the nnViwer GUI\n",
    "run_gui(wrapped_model.graph)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
